{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "066c1f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b309ae3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openpyxl\n",
    "%pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcf0c3d",
   "metadata": {},
   "source": [
    "-------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72681427",
   "metadata": {},
   "source": [
    "Prepare data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2392e481",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Load the climate data\n",
    "\n",
    "file_path = \"Kalam Climate Data.xlsx\"   \n",
    "climate_df = pd.read_excel(file_path)\n",
    "\n",
    "climate_df['Date Time'] = pd.to_datetime(climate_df['Date Time'])\n",
    "\n",
    "# Extract date\n",
    "\n",
    "climate_df['date'] = climate_df['Date Time'].dt.date\n",
    "\n",
    "# Daily aggregation\n",
    "daily_climate = climate_df.groupby('date').agg({\n",
    "    'Temperature (°C)': ['mean', 'min', 'max'],\n",
    "    'Dewpoint Temperature (°C)': ['mean', 'min', 'max'],\n",
    "    'U Wind Component (m/s)': 'mean',\n",
    "    'V Wind Component (m/s)': 'mean',\n",
    "    'Total Precipitation (mm)': 'sum',\n",
    "    'Snowfall (mm)': 'sum',\n",
    "    'Snow Cover (%)': 'mean'\n",
    "})\n",
    "\n",
    "# Flatten columns\n",
    "daily_climate.columns = ['_'.join(col).strip() for col in daily_climate.columns.values]\n",
    "\n",
    "# Reset index\n",
    "daily_climate = daily_climate.reset_index()\n",
    "\n",
    "# Define start and end dates\n",
    "start_date = pd.to_datetime('2024-09-24')\n",
    "end_date   = pd.to_datetime('2024-10-24')\n",
    "\n",
    "daily_climate[\"date\"] = pd.to_datetime(daily_climate[\"date\"])\n",
    "\n",
    "# Filter DataFrame\n",
    "extra_month_df = daily_climate[(daily_climate['date'] >= start_date) & (daily_climate['date'] <= end_date)]\n",
    "\n",
    "\n",
    "new_names = {\n",
    "    'date': 'Date',\n",
    "    'Temperature (°C)_mean': 'Temp_Mean',\n",
    "    'Temperature (°C)_min': 'Temp_Min',\n",
    "    'Temperature (°C)_max': 'Temp_Max',\n",
    "    'Dewpoint Temperature (°C)_mean': 'Dewpoint_Mean',\n",
    "    'Dewpoint Temperature (°C)_min': 'Dewpoint_Min',\n",
    "    'Dewpoint Temperature (°C)_max': 'Dewpoint_Max',\n",
    "    'U Wind Component (m/s)_mean': 'U_Wind_Mean',\n",
    "    'V Wind Component (m/s)_mean': 'V_Wind_Mean',\n",
    "    'Total Precipitation (mm)_sum': 'Precipitation_Sum',\n",
    "    'Snowfall (mm)_sum': 'Snowfall_Sum',\n",
    "    'Snow Cover (%)_mean': 'SnowCover_Mean'\n",
    "}\n",
    "\n",
    "\n",
    "extra_month_df.rename(columns=new_names, inplace=True)\n",
    "\n",
    "all_data_df = pd.read_csv(\"Data.csv\")\n",
    "\n",
    "all_data_df.drop(columns=[\"consumer_device_9\", \"consumer_device_x\", \"v_red\", \"v_blue\",\"v_yellow\", \"current\", \"power_factor\"], inplace=True)\n",
    "all_data_df.head()\n",
    "\n",
    "# Ensure datetime\n",
    "all_data_df['date_time'] = pd.to_datetime(all_data_df['date_time'])\n",
    "\n",
    "\n",
    "# Extract date (drop time)\n",
    "all_data_df['date'] = all_data_df['date_time'].dt.date\n",
    "\n",
    "\n",
    "# Group by Source (consumer_device + data_user) and date\n",
    "\n",
    "daily_df = all_data_df.groupby(['Source', 'date']).agg({\n",
    "    'kwh': 'sum'  \n",
    "})\n",
    "\n",
    "daily_df = daily_df.reset_index()\n",
    "\n",
    "\n",
    "# Ensure datetime index\n",
    "daily_df = daily_df.set_index(\"date\").sort_index()\n",
    "daily_climate = daily_climate.set_index(\"date\").sort_index()\n",
    "\n",
    "# Merge\n",
    "merged_daily_df = daily_df.join(daily_climate, how=\"left\")\n",
    "\n",
    "\n",
    "merged_daily_df.reset_index(inplace=True)\n",
    "\n",
    "merged_daily_df.to_csv(\"second_daily_merged_hydro_climate.csv\", index=False)\n",
    "\n",
    "\n",
    "# Dictionary mapping old names to new names\n",
    "new_names = {\n",
    "    'date': 'Date',\n",
    "    'Source': 'Source',\n",
    "    'kwh': 'kwh',\n",
    "    'Temperature (°C)_mean': 'Temp_Mean',\n",
    "    'Temperature (°C)_min': 'Temp_Min',\n",
    "    'Temperature (°C)_max': 'Temp_Max',\n",
    "    'Dewpoint Temperature (°C)_mean': 'Dewpoint_Mean',\n",
    "    'Dewpoint Temperature (°C)_min': 'Dewpoint_Min',\n",
    "    'Dewpoint Temperature (°C)_max': 'Dewpoint_Max',\n",
    "    'U Wind Component (m/s)_mean': 'U_Wind_Mean',\n",
    "    'V Wind Component (m/s)_mean': 'V_Wind_Mean',\n",
    "    'Total Precipitation (mm)_sum': 'Precipitation_Sum',\n",
    "    'Snowfall (mm)_sum': 'Snowfall_Sum',\n",
    "    'Snow Cover (%)_mean': 'SnowCover_Mean'\n",
    "}\n",
    "\n",
    "merged_daily_df.rename(columns=new_names, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18bce919",
   "metadata": {},
   "source": [
    "----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bc22c1",
   "metadata": {},
   "source": [
    "# Let us train a Feed-Forward Neural Network (MLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64c9aa0",
   "metadata": {},
   "source": [
    "# 1 st Approach: -----> Zindi Score 4.928191196 kWh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9acd9c4",
   "metadata": {},
   "source": [
    "Feature Engineering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2dce658",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features(df):\n",
    "    df = df.copy()\n",
    "    df[\"Temp_dew_diff\"] = df[\"Temp_Mean\"] - df[\"Dewpoint_Mean\"]\n",
    "    df[\"wind_speed\"] = np.sqrt(df[\"U_Wind_Mean\"]**2 + df[\"V_Wind_Mean\"]**2)\n",
    "    df[\"precip_snow_ratio\"] = df[\"Precipitation_Sum\"] / (df[\"Snowfall_Sum\"] + 1e-6)\n",
    "    return df\n",
    "\n",
    "def add_lag_roll(df, group_col=\"Source\", target_col=\"kwh\", lags=[1,2,7], windows=[3,7]):\n",
    "    df = df.sort_values([\"Source\",\"Date\"]).copy()\n",
    "    for lag in lags:\n",
    "        df[f\"lag_{lag}\"] = df.groupby(group_col)[target_col].shift(lag)\n",
    "    for w in windows:\n",
    "        df[f\"roll_mean_{w}\"] = df.groupby(group_col)[target_col].shift(1).rolling(w).mean()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c19c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Reindex all Sources across full date range ---\n",
    "def reindex_sources(df):\n",
    "    all_dates = pd.date_range(df[\"Date\"].min(), df[\"Date\"].max())\n",
    "    all_sources = df[\"Source\"].unique()\n",
    "    idx = pd.MultiIndex.from_product([all_sources, all_dates], names=[\"Source\",\"Date\"])\n",
    "    df_full = df.set_index([\"Source\",\"Date\"]).reindex(idx).reset_index()\n",
    "    df_full[\"kwh\"] = df_full[\"kwh\"].fillna(0)\n",
    "    climate_cols = [c for c in df.columns if c not in [\"Source\",\"kwh\",\"Date\"]]\n",
    "    for c in climate_cols:\n",
    "        df_full[c] = df_full.groupby(\"Date\")[c].transform(\"first\")\n",
    "    return df_full\n",
    "\n",
    "\n",
    "# --- Prepare dataset ---\n",
    "df = merged_daily_df.copy()\n",
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "df = reindex_sources(df)\n",
    "df = add_features(df)\n",
    "df = add_lag_roll(df)\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "feature_cols = [\n",
    "    \"Temp_Mean\",\"Temp_Min\",\"Temp_Max\",\"Dewpoint_Mean\",\"Dewpoint_Min\",\"Dewpoint_Max\",\n",
    "    \"U_Wind_Mean\",\"V_Wind_Mean\",\"Precipitation_Sum\",\"Snowfall_Sum\",\"SnowCover_Mean\",\n",
    "    \"Temp_dew_diff\",\"wind_speed\",\"precip_snow_ratio\",\n",
    "] + [c for c in df.columns if c.startswith(\"lag_\") or c.startswith(\"roll_\")]\n",
    "\n",
    "X = df[feature_cols].values\n",
    "y = df[\"kwh\"].values\n",
    "\n",
    "\n",
    "\n",
    "# Scale features for neural net\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff603fb",
   "metadata": {},
   "source": [
    "Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b6462f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# --- Build MLP model ---\n",
    "def create_mlp(input_dim):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(128, activation=\"relu\", input_dim=input_dim))\n",
    "    model.add(layers.Dense(64, activation=\"relu\"))\n",
    "    model.add(layers.Dense(32, activation=\"relu\"))\n",
    "    model.add(layers.Dense(1, activation=\"relu\"))  # relu ensures no negative kwh\n",
    "    model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
    "    return model\n",
    "\n",
    "mlp = create_mlp(X_train.shape[1])\n",
    "\n",
    "# --- Callbacks: EarlyStopping + ReduceLROnPlateau ---\n",
    "early_stop = callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\", patience=5, restore_best_weights=True, verbose=1\n",
    ")\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\", factor=0.5, patience=3, min_lr=1e-6, verbose=1\n",
    ")\n",
    "\n",
    "history = mlp.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test,y_test),\n",
    "    epochs=100, batch_size=256, verbose=1,\n",
    "    callbacks=[early_stop, reduce_lr]\n",
    ")\n",
    "\n",
    "# Predict on test\n",
    "y_pred = mlp.predict(X_test).flatten()\n",
    "y_pred = np.maximum(0, y_pred)  # safety clip\n",
    "\n",
    "# --- Global Metrics ---\n",
    "print(\"=== Global Metrics ===\")\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "print(f\"Regression: RMSE={rmse:.3f}, MAPE={mape:.3f}\\n\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5447819",
   "metadata": {},
   "source": [
    "Predict next month (extra climate data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a17a269",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "extra = add_features(extra_month_df.copy())\n",
    "extra[\"Date\"] = pd.to_datetime(extra[\"Date\"])\n",
    "\n",
    "preds = []\n",
    "sources = df[\"Source\"].unique()\n",
    "for src in sources:\n",
    "    hist = df[df[\"Source\"]==src].copy()\n",
    "    extra_src = extra.copy()\n",
    "    extra_src[\"Source\"] = src\n",
    "    lag_hist = hist.tail(7).reset_index(drop=True)\n",
    "    rows = []\n",
    "    for _, row in extra_src.iterrows():\n",
    "        r = row.to_dict()\n",
    "        for lag in [1,2,7]:\n",
    "            if len(lag_hist) >= lag:\n",
    "                r[f\"lag_{lag}\"] = lag_hist.loc[len(lag_hist)-lag,\"kwh\"]\n",
    "            else:\n",
    "                r[f\"lag_{lag}\"] = np.nan\n",
    "        for w in [3,7]:\n",
    "            if len(lag_hist) >= w:\n",
    "                r[f\"roll_mean_{w}\"] = lag_hist[\"kwh\"].iloc[-w:].mean()\n",
    "            else:\n",
    "                r[f\"roll_mean_{w}\"] = lag_hist[\"kwh\"].mean()\n",
    "        X_ex = pd.DataFrame([r])[feature_cols].fillna(0)\n",
    "        X_ex = scaler.transform(X_ex)\n",
    "        r[\"pred_kwh\"] = max(0, mlp.predict(X_ex)[0,0])\n",
    "        lag_hist = pd.concat([lag_hist, pd.DataFrame([{\"kwh\": r[\"pred_kwh\"]}])], ignore_index=True)\n",
    "        rows.append(r)\n",
    "    preds.append(pd.DataFrame(rows))\n",
    "preds_df = pd.concat(preds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8c3bed",
   "metadata": {},
   "source": [
    "Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44eaff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Per Source\n",
    "\n",
    "def plot_source(src):\n",
    "    plt.figure(figsize=(12,4))\n",
    "    d = df[df[\"Source\"]==src]\n",
    "    plt.plot(d[\"Date\"], d[\"kwh\"], label=\"actual\")\n",
    "    d_extra = preds_df[preds_df[\"Source\"]==src]\n",
    "    plt.plot(d_extra[\"Date\"], d_extra[\"pred_kwh\"], \"--\", label=\"pred (extra month)\")\n",
    "    plt.title(f\"Source {src}\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "for src in sources:\n",
    "    plot_source(src)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4535976a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall \n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(df.groupby(\"Date\")[\"kwh\"].sum(), label=\"actual total\")\n",
    "plt.plot(preds_df.groupby(\"Date\")[\"pred_kwh\"].sum(), \"--\", label=\"pred total (extra month)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d58cbc",
   "metadata": {},
   "source": [
    "Build Submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04318d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv(\"SampleSubmission.csv\")\n",
    "sub[\"Date\"] = sub[\"ID\"].apply(lambda x: x.split(\"_\")[0])\n",
    "sub[\"Source\"] = sub[\"ID\"].apply(lambda x: \"_\".join(x.split(\"_\")[1:]))\n",
    "sub[\"Date\"] = pd.to_datetime(sub[\"Date\"])\n",
    "\n",
    "preds_out = preds_df.copy()\n",
    "preds_out[\"Date\"] = pd.to_datetime(preds_out[\"Date\"])\n",
    "\n",
    "submission = sub.merge(preds_out[[\"Date\",\"Source\",\"pred_kwh\"]], on=[\"Date\",\"Source\"], how=\"left\")\n",
    "submission = submission[[\"ID\",\"pred_kwh\"]]\n",
    "\n",
    "submission.to_csv(\"MySubmission_3_MLP.csv\", index=False)\n",
    "submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b330a150",
   "metadata": {},
   "source": [
    "-------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee9a1f1",
   "metadata": {},
   "source": [
    "# Second Approach:   performed worst ----> Zindi 6.431916951\n",
    "\n",
    "1) Add calendar features + longer rolling windows:\n",
    "\n",
    "- Calendar features (day_of_week, month, is_weekend, season)\n",
    "\n",
    "- Longer rolling windows (14 and 30 days)\n",
    "\n",
    "- Exponential moving average (EMA) features\n",
    "\n",
    "2) Added Dropout (0.2) layers for better generalization.\n",
    "\n",
    "3) Extended lag history to 30 days when predicting extra month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d4e06a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m821/821\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 10ms/step - loss: 2.2619 - root_mean_squared_error: 1.5040 - val_loss: 1.7733 - val_root_mean_squared_error: 1.3317 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "\u001b[1m821/821\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - loss: 1.8603 - root_mean_squared_error: 1.3639 - val_loss: 1.6089 - val_root_mean_squared_error: 1.2684 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "\u001b[1m821/821\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 8ms/step - loss: 1.8401 - root_mean_squared_error: 1.3565 - val_loss: 1.7014 - val_root_mean_squared_error: 1.3044 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "\u001b[1m821/821\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - loss: 1.8309 - root_mean_squared_error: 1.3531 - val_loss: 1.6745 - val_root_mean_squared_error: 1.2940 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "\u001b[1m820/821\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1.7012 - root_mean_squared_error: 1.3011\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m821/821\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 12ms/step - loss: 1.8223 - root_mean_squared_error: 1.3499 - val_loss: 1.7380 - val_root_mean_squared_error: 1.3183 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "\u001b[1m821/821\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 12ms/step - loss: 1.6464 - root_mean_squared_error: 1.2831 - val_loss: 1.5661 - val_root_mean_squared_error: 1.2514 - learning_rate: 5.0000e-04\n",
      "Epoch 7/100\n",
      "\u001b[1m821/821\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 12ms/step - loss: 1.6121 - root_mean_squared_error: 1.2697 - val_loss: 1.6820 - val_root_mean_squared_error: 1.2969 - learning_rate: 5.0000e-04\n",
      "Epoch 8/100\n",
      "\u001b[1m821/821\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - loss: 1.6286 - root_mean_squared_error: 1.2762 - val_loss: 1.6216 - val_root_mean_squared_error: 1.2734 - learning_rate: 5.0000e-04\n",
      "Epoch 9/100\n",
      "\u001b[1m815/821\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 1.6317 - root_mean_squared_error: 1.2767\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m821/821\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - loss: 1.6283 - root_mean_squared_error: 1.2761 - val_loss: 1.6693 - val_root_mean_squared_error: 1.2920 - learning_rate: 5.0000e-04\n",
      "Epoch 10/100\n",
      "\u001b[1m821/821\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - loss: 1.5715 - root_mean_squared_error: 1.2536 - val_loss: 1.4987 - val_root_mean_squared_error: 1.2242 - learning_rate: 2.5000e-04\n",
      "Epoch 11/100\n",
      "\u001b[1m821/821\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - loss: 1.5236 - root_mean_squared_error: 1.2344 - val_loss: 1.5224 - val_root_mean_squared_error: 1.2339 - learning_rate: 2.5000e-04\n",
      "Epoch 12/100\n",
      "\u001b[1m821/821\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - loss: 1.4913 - root_mean_squared_error: 1.2212 - val_loss: 1.5234 - val_root_mean_squared_error: 1.2343 - learning_rate: 2.5000e-04\n",
      "Epoch 13/100\n",
      "\u001b[1m818/821\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.5414 - root_mean_squared_error: 1.2409\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m821/821\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - loss: 1.4842 - root_mean_squared_error: 1.2183 - val_loss: 1.5815 - val_root_mean_squared_error: 1.2576 - learning_rate: 2.5000e-04\n",
      "Epoch 14/100\n",
      "\u001b[1m821/821\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - loss: 1.4484 - root_mean_squared_error: 1.2035 - val_loss: 1.4688 - val_root_mean_squared_error: 1.2119 - learning_rate: 1.2500e-04\n",
      "Epoch 15/100\n",
      "\u001b[1m821/821\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 9ms/step - loss: 1.4585 - root_mean_squared_error: 1.2077 - val_loss: 1.4608 - val_root_mean_squared_error: 1.2086 - learning_rate: 1.2500e-04\n",
      "Epoch 16/100\n",
      "\u001b[1m821/821\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 12ms/step - loss: 1.4372 - root_mean_squared_error: 1.1988 - val_loss: 1.4730 - val_root_mean_squared_error: 1.2137 - learning_rate: 1.2500e-04\n",
      "Epoch 17/100\n",
      "\u001b[1m821/821\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 14ms/step - loss: 1.4434 - root_mean_squared_error: 1.2014 - val_loss: 1.4418 - val_root_mean_squared_error: 1.2008 - learning_rate: 1.2500e-04\n",
      "Epoch 18/100\n",
      "\u001b[1m821/821\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 12ms/step - loss: 1.3799 - root_mean_squared_error: 1.1747 - val_loss: 1.4505 - val_root_mean_squared_error: 1.2044 - learning_rate: 1.2500e-04\n",
      "Epoch 19/100\n",
      "\u001b[1m821/821\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - loss: 1.4178 - root_mean_squared_error: 1.1907 - val_loss: 1.4308 - val_root_mean_squared_error: 1.1961 - learning_rate: 1.2500e-04\n",
      "Epoch 20/100\n",
      "\u001b[1m821/821\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 12ms/step - loss: 1.4043 - root_mean_squared_error: 1.1850 - val_loss: 1.4549 - val_root_mean_squared_error: 1.2062 - learning_rate: 1.2500e-04\n",
      "Epoch 21/100\n",
      "\u001b[1m821/821\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - loss: 1.4646 - root_mean_squared_error: 1.2102 - val_loss: 1.4683 - val_root_mean_squared_error: 1.2117 - learning_rate: 1.2500e-04\n",
      "Epoch 22/100\n",
      "\u001b[1m817/821\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 1.3248 - root_mean_squared_error: 1.1506\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\u001b[1m821/821\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - loss: 1.3555 - root_mean_squared_error: 1.1643 - val_loss: 1.4322 - val_root_mean_squared_error: 1.1967 - learning_rate: 1.2500e-04\n",
      "Epoch 23/100\n",
      "\u001b[1m821/821\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - loss: 1.3941 - root_mean_squared_error: 1.1807 - val_loss: 1.4606 - val_root_mean_squared_error: 1.2086 - learning_rate: 6.2500e-05\n",
      "Epoch 24/100\n",
      "\u001b[1m821/821\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - loss: 1.4072 - root_mean_squared_error: 1.1863 - val_loss: 1.4391 - val_root_mean_squared_error: 1.1996 - learning_rate: 6.2500e-05\n",
      "Epoch 24: early stopping\n",
      "Restoring model weights from the end of the best epoch: 19.\n",
      "\u001b[1m1642/1642\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step\n",
      "=== Global Metrics ===\n",
      "Regression: RMSE=1.196, MAPE=71829063279201.969\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Feature Engineering \n",
    "\n",
    "\n",
    "def add_features(df):\n",
    "    df = df.copy()\n",
    "    # Climate derived features\n",
    "    df[\"Temp_dew_diff\"] = df[\"Temp_Mean\"] - df[\"Dewpoint_Mean\"]\n",
    "    df[\"wind_speed\"] = np.sqrt(df[\"U_Wind_Mean\"]**2 + df[\"V_Wind_Mean\"]**2)\n",
    "    df[\"precip_snow_ratio\"] = df[\"Precipitation_Sum\"] / (df[\"Snowfall_Sum\"] + 1e-6)\n",
    "    \n",
    "    # Calendar features\n",
    "    df[\"day_of_week\"] = df[\"Date\"].dt.dayofweek\n",
    "    df[\"month\"] = df[\"Date\"].dt.month\n",
    "    df[\"is_weekend\"] = (df[\"day_of_week\"] >= 5).astype(int)\n",
    "    # Simple season encoding (1–4)\n",
    "    df[\"season\"] = ((df[\"month\"] % 12 + 3) // 3)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def add_lag_roll(df, group_col=\"Source\", target_col=\"kwh\", lags=[1,2,7], windows=[3,7,14,30]):\n",
    "    df = df.sort_values([group_col,\"Date\"]).copy()\n",
    "    # Lags\n",
    "    for lag in lags:\n",
    "        df[f\"lag_{lag}\"] = df.groupby(group_col)[target_col].shift(lag)\n",
    "    # Rolling means\n",
    "    for w in windows:\n",
    "        df[f\"roll_mean_{w}\"] = df.groupby(group_col)[target_col].shift(1).rolling(w).mean()\n",
    "    # Exponential moving averages\n",
    "    for span in [7,14,30]:\n",
    "        df[f\"ema_{span}\"] = df.groupby(group_col)[target_col].shift(1).ewm(span=span, adjust=False).mean()\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_source_stats(df, group_col=\"Source\", target_col=\"kwh\"):\n",
    "    stats = df.groupby(group_col)[target_col].agg([\"mean\",\"median\"]).reset_index()\n",
    "    stats.columns = [group_col, \"src_mean_kwh\", \"src_median_kwh\"]\n",
    "    df = df.merge(stats, on=group_col, how=\"left\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def reindex_sources(df):\n",
    "    all_dates = pd.date_range(df[\"Date\"].min(), df[\"Date\"].max())\n",
    "    all_sources = df[\"Source\"].unique()\n",
    "    idx = pd.MultiIndex.from_product([all_sources, all_dates], names=[\"Source\",\"Date\"])\n",
    "    df_full = df.set_index([\"Source\",\"Date\"]).reindex(idx).reset_index()\n",
    "    df_full[\"kwh\"] = df_full[\"kwh\"].fillna(0)\n",
    "    climate_cols = [c for c in df.columns if c not in [\"Source\",\"kwh\",\"Date\"]]\n",
    "    for c in climate_cols:\n",
    "        df_full[c] = df_full.groupby(\"Date\")[c].transform(\"first\")\n",
    "    return df_full\n",
    "\n",
    "\n",
    "\n",
    "#  Prepare dataset \n",
    "\n",
    "\n",
    "df = merged_daily_df.copy()\n",
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "df = reindex_sources(df)\n",
    "df = add_features(df)\n",
    "df = add_lag_roll(df)\n",
    "df = add_source_stats(df)\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "# Features\n",
    "feature_cols = [\n",
    "    \"Temp_Mean\",\"Temp_Min\",\"Temp_Max\",\"Dewpoint_Mean\",\"Dewpoint_Min\",\"Dewpoint_Max\",\n",
    "    \"U_Wind_Mean\",\"V_Wind_Mean\",\"Precipitation_Sum\",\"Snowfall_Sum\",\"SnowCover_Mean\",\n",
    "    \"Temp_dew_diff\",\"wind_speed\",\"precip_snow_ratio\",\n",
    "    \"day_of_week\",\"month\",\"is_weekend\",\"season\",\n",
    "    \"src_mean_kwh\",\"src_median_kwh\"\n",
    "] + [c for c in df.columns if c.startswith(\"lag_\") or c.startswith(\"roll_\") or c.startswith(\"ema_\")]\n",
    "\n",
    "X = df[feature_cols].values\n",
    "y = df[\"kwh\"].values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "#  Build MLP model \n",
    "\n",
    "\n",
    "def create_mlp(input_dim):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(128, activation=\"relu\", input_dim=input_dim))\n",
    "    model.add(layers.Dropout(0.2))\n",
    "    model.add(layers.Dense(64, activation=\"relu\"))\n",
    "    model.add(layers.Dropout(0.2))\n",
    "    model.add(layers.Dense(32, activation=\"relu\"))\n",
    "    model.add(layers.Dense(1, activation=\"relu\"))                                                         # Here relu ensures no negative predictions, NO NEED TO WORRY ABOUT IT !\n",
    "    model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
    "    return model\n",
    "\n",
    "mlp = create_mlp(X_train.shape[1])\n",
    "\n",
    "early_stop = callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\", patience=5, restore_best_weights=True, verbose=1\n",
    ")\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\", factor=0.5, patience=3, min_lr=1e-6, verbose=1\n",
    ")\n",
    "\n",
    "history = mlp.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=100, batch_size=256, verbose=1,\n",
    "    callbacks=[early_stop, reduce_lr]\n",
    ")\n",
    "\n",
    "\n",
    "#  Evaluate on test set \n",
    "\n",
    "y_pred = mlp.predict(X_test).flatten()\n",
    "y_pred = np.maximum(0, y_pred)\n",
    "\n",
    "print(\"=== Global Metrics ===\")\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "print(f\"Regression: RMSE={rmse:.3f}, MAPE={mape:.3f}\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e330c2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Predict extra month \n",
    "\n",
    "extra = add_features(extra_month_df.copy())\n",
    "extra[\"Date\"] = pd.to_datetime(extra[\"Date\"])\n",
    "\n",
    "preds = []\n",
    "sources = df[\"Source\"].unique()\n",
    "for src in sources:\n",
    "    hist = df[df[\"Source\"]==src].copy()\n",
    "    extra_src = extra.copy()\n",
    "    extra_src[\"Source\"] = src\n",
    "    lag_hist = hist.tail(30).reset_index(drop=True)  # keep last 30 days\n",
    "    rows = []\n",
    "    for _, row in extra_src.iterrows():\n",
    "        r = row.to_dict()\n",
    "        # Lags\n",
    "        for lag in [1,2,7]:\n",
    "            if len(lag_hist) >= lag:\n",
    "                r[f\"lag_{lag}\"] = lag_hist.loc[len(lag_hist)-lag, \"kwh\"]\n",
    "            else:\n",
    "                r[f\"lag_{lag}\"] = 0\n",
    "        # Rolling means\n",
    "        for w in [3,7,14,30]:\n",
    "            if len(lag_hist) >= w:\n",
    "                r[f\"roll_mean_{w}\"] = lag_hist[\"kwh\"].iloc[-w:].mean()\n",
    "            else:\n",
    "                r[f\"roll_mean_{w}\"] = lag_hist[\"kwh\"].mean()\n",
    "        # EMA\n",
    "        for span in [7,14,30]:\n",
    "            r[f\"ema_{span}\"] = lag_hist[\"kwh\"].ewm(span=span, adjust=False).mean().iloc[-1]\n",
    "        # Source stats\n",
    "        r[\"src_mean_kwh\"] = hist[\"kwh\"].mean()\n",
    "        r[\"src_median_kwh\"] = hist[\"kwh\"].median()\n",
    "        # Predict\n",
    "        X_ex = pd.DataFrame([r])[feature_cols].fillna(0)\n",
    "        X_ex = scaler.transform(X_ex)\n",
    "        r[\"pred_kwh\"] = max(0, mlp.predict(X_ex, verbose=0)[0,0])\n",
    "        lag_hist = pd.concat([lag_hist, pd.DataFrame([{\"kwh\": r[\"pred_kwh\"]}])], ignore_index=True)\n",
    "        rows.append(r)\n",
    "    preds.append(pd.DataFrame(rows))\n",
    "preds_df = pd.concat(preds)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f1d863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Submission saved to MySubmission_ImprovedMLP.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Prepare submission \n",
    "\n",
    "sub = pd.read_csv(\"SampleSubmission.csv\")\n",
    "sub[\"Date\"] = sub[\"ID\"].apply(lambda x: x.split(\"_\")[0])\n",
    "sub[\"Source\"] = sub[\"ID\"].apply(lambda x: \"_\".join(x.split(\"_\")[1:]))\n",
    "sub[\"Date\"] = pd.to_datetime(sub[\"Date\"])\n",
    "\n",
    "preds_out = preds_df.copy()\n",
    "preds_out[\"Date\"] = pd.to_datetime(preds_out[\"Date\"])\n",
    "\n",
    "submission = sub.merge(preds_out[[\"Date\",\"Source\",\"pred_kwh\"]], on=[\"Date\",\"Source\"], how=\"left\")\n",
    "submission = submission[[\"ID\",\"pred_kwh\"]]\n",
    "submission.to_csv(\"MySubmission_5_MLP.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
