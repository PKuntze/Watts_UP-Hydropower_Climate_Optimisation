{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0f0ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from prophet import Prophet\n",
    "from math import sqrt\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75487d9",
   "metadata": {},
   "source": [
    "## **Exploratory Data Analysis (EDA)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12512af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_df = pd.read_csv('Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdbd420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure datetime\n",
    "all_data_df['date_time'] = pd.to_datetime(all_data_df['date_time'])\n",
    "all_data_df = all_data_df.sort_values(by='date_time').reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e4f022",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_df.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9de19d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c41af6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count how many phase columns are non-null per row\n",
    "all_data_df['active_phases'] = all_data_df[['v_red', 'v_blue', 'v_yellow']].notnull().sum(axis=1)\n",
    "\n",
    "# Check distribution of active phases\n",
    "phase_counts = all_data_df['active_phases'].value_counts().sort_index()\n",
    "print(\"Number of active phases per row:\")\n",
    "print(phase_counts)\n",
    "\n",
    "# Count rows where none of the phases are active\n",
    "num_inactive = (all_data_df['active_phases'] == 0).sum()\n",
    "print(f\"\\nNumber of rows with no active phases (all NaN): {num_inactive}\")\n",
    "\n",
    "# Check if single-phase data\n",
    "if phase_counts.index.max() == 1:\n",
    "    print(\"\\nConfirmed: Each row has exactly one active phase (single-phase recording).\")\n",
    "else:\n",
    "    print(\"\\nSome rows have more than one phase active at the same time.\")\n",
    "\n",
    "# Check rows with more than 1 active phase\n",
    "multi_phase_rows = all_data_df[all_data_df['active_phases'] > 1]\n",
    "if not multi_phase_rows.empty:\n",
    "    print(\"\\nRows with multiple active phases detected:\")\n",
    "    print(multi_phase_rows[['date_time','v_red','v_blue','v_yellow']].head(10))\n",
    "\n",
    "# Check rows with zero active phases\n",
    "inactive_rows = all_data_df[all_data_df['active_phases'] == 0]\n",
    "if not inactive_rows.empty:\n",
    "    print(\"\\nRows with no active phases (all NaN):\")\n",
    "    print(inactive_rows[['date_time','v_red','v_blue','v_yellow']].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee5678b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_df = all_data_df.copy()\n",
    "\n",
    "# Datetime to sort\n",
    "all_data_df = all_data_df.sort_values('date_time').reset_index(drop=True)\n",
    "\n",
    "# Time in hours\n",
    "all_data_df['time_delta_hours'] = all_data_df['date_time'].diff().dt.total_seconds() / 3600\n",
    "\n",
    "# Safely set the first row\n",
    "all_data_df.loc[0, 'time_delta_hours'] = 5 / 60  # 0.08333 hours\n",
    "\n",
    "# Extreme deltas\n",
    "all_data_df['time_delta_hours'] = all_data_df['time_delta_hours'].clip(lower=0.001, upper=0.2)\n",
    "\n",
    "# Estimated kWh for each interval\n",
    "all_data_df['kwh_estimated'] = (all_data_df['active_phases'] * all_data_df['current'] * all_data_df['power_factor'] * all_data_df['time_delta_hours']) / 1000\n",
    "\n",
    "# Residuals\n",
    "all_data_df['residual'] = all_data_df['kwh'] - all_data_df['kwh_estimated']\n",
    "\n",
    "# Check matches\n",
    "tolerance = 0.01\n",
    "all_data_df['match'] = np.isclose(all_data_df['kwh'], all_data_df['kwh_estimated'], atol=tolerance)\n",
    "\n",
    "num_correct = all_data_df['match'].sum()\n",
    "num_incorrect = (~all_data_df['match']).sum()\n",
    "\n",
    "print(f\"Number of rows where formula is correct: {num_correct}\")\n",
    "print(f\"Number of rows where formula is incorrect: {num_incorrect}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13123544",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Load the climate data\n",
    "\n",
    "file_path = \"Kalam Climate Data.xlsx\"   \n",
    "climate_df = pd.read_excel(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3f2138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure datetime\n",
    "climate_df['Date Time'] = pd.to_datetime(climate_df['Date Time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0482e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "climate_df.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50599435",
   "metadata": {},
   "outputs": [],
   "source": [
    "climate_df.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da161c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "climate_df.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c8239a",
   "metadata": {},
   "source": [
    "### Merging climate and power datasets. Moving from hourly to Daily forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363b8f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Load the climate data\n",
    "\n",
    "file_path = \"Kalam Climate Data.xlsx\"   \n",
    "climate_df = pd.read_excel(file_path)\n",
    "\n",
    "climate_df['Date Time'] = pd.to_datetime(climate_df['Date Time'])\n",
    "\n",
    "# Extract date\n",
    "\n",
    "climate_df['date'] = climate_df['Date Time'].dt.date\n",
    "\n",
    "# Daily aggregation\n",
    "daily_climate = climate_df.groupby('date').agg({\n",
    "    'Temperature (°C)': ['mean', 'min', 'max'],\n",
    "    'Dewpoint Temperature (°C)': ['mean', 'min', 'max'],\n",
    "    'U Wind Component (m/s)': 'mean',\n",
    "    'V Wind Component (m/s)': 'mean',\n",
    "    'Total Precipitation (mm)': 'sum',\n",
    "    'Snowfall (mm)': 'sum',\n",
    "    'Snow Cover (%)': 'mean'\n",
    "})\n",
    "\n",
    "# Flatten columns\n",
    "daily_climate.columns = ['_'.join(col).strip() for col in daily_climate.columns.values]\n",
    "\n",
    "# Reset index\n",
    "daily_climate = daily_climate.reset_index()\n",
    "\n",
    "# Define start and end dates\n",
    "start_date = pd.to_datetime('2024-09-24')\n",
    "end_date   = pd.to_datetime('2024-10-24')\n",
    "\n",
    "daily_climate[\"date\"] = pd.to_datetime(daily_climate[\"date\"])\n",
    "\n",
    "# Filter DataFrame\n",
    "extra_month_df = daily_climate[(daily_climate['date'] >= start_date) & (daily_climate['date'] <= end_date)]\n",
    "\n",
    "\n",
    "new_names = {\n",
    "    'date': 'Date',\n",
    "    'Temperature (°C)_mean': 'Temp_Mean',\n",
    "    'Temperature (°C)_min': 'Temp_Min',\n",
    "    'Temperature (°C)_max': 'Temp_Max',\n",
    "    'Dewpoint Temperature (°C)_mean': 'Dewpoint_Mean',\n",
    "    'Dewpoint Temperature (°C)_min': 'Dewpoint_Min',\n",
    "    'Dewpoint Temperature (°C)_max': 'Dewpoint_Max',\n",
    "    'U Wind Component (m/s)_mean': 'U_Wind_Mean',\n",
    "    'V Wind Component (m/s)_mean': 'V_Wind_Mean',\n",
    "    'Total Precipitation (mm)_sum': 'Precipitation_Sum',\n",
    "    'Snowfall (mm)_sum': 'Snowfall_Sum',\n",
    "    'Snow Cover (%)_mean': 'SnowCover_Mean'\n",
    "}\n",
    "\n",
    "\n",
    "extra_month_df.rename(columns=new_names, inplace=True)\n",
    "\n",
    "all_data_df = pd.read_csv(\"Data.csv\")\n",
    "\n",
    "all_data_df.drop(columns=[\"consumer_device_9\", \"consumer_device_x\", \"v_red\", \"v_blue\",\"v_yellow\", \"current\", \"power_factor\"], inplace=True)\n",
    "all_data_df.head()\n",
    "\n",
    "# Ensure datetime\n",
    "all_data_df['date_time'] = pd.to_datetime(all_data_df['date_time'])\n",
    "\n",
    "\n",
    "# Extract date (drop time)\n",
    "all_data_df['date'] = all_data_df['date_time'].dt.date\n",
    "\n",
    "\n",
    "# Group by Source (consumer_device + data_user) and date\n",
    "\n",
    "daily_df = all_data_df.groupby(['Source', 'date']).agg({\n",
    "    'kwh': 'sum'  \n",
    "})\n",
    "\n",
    "daily_df = daily_df.reset_index()\n",
    "\n",
    "\n",
    "# Ensure datetime index\n",
    "daily_df = daily_df.set_index(\"date\").sort_index()\n",
    "daily_climate = daily_climate.set_index(\"date\").sort_index()\n",
    "\n",
    "# Merge\n",
    "merged_daily_df = daily_df.join(daily_climate, how=\"left\")\n",
    "\n",
    "\n",
    "merged_daily_df.reset_index(inplace=True)\n",
    "\n",
    "merged_daily_df.to_csv(\"second_daily_merged_hydro_climate.csv\", index=False)\n",
    "\n",
    "\n",
    "# Dictionary mapping old names to new names\n",
    "new_names = {\n",
    "    'date': 'Date',\n",
    "    'Source': 'Source',\n",
    "    'kwh': 'kwh',\n",
    "    'Temperature (°C)_mean': 'Temp_Mean',\n",
    "    'Temperature (°C)_min': 'Temp_Min',\n",
    "    'Temperature (°C)_max': 'Temp_Max',\n",
    "    'Dewpoint Temperature (°C)_mean': 'Dewpoint_Mean',\n",
    "    'Dewpoint Temperature (°C)_min': 'Dewpoint_Min',\n",
    "    'Dewpoint Temperature (°C)_max': 'Dewpoint_Max',\n",
    "    'U Wind Component (m/s)_mean': 'U_Wind_Mean',\n",
    "    'V Wind Component (m/s)_mean': 'V_Wind_Mean',\n",
    "    'Total Precipitation (mm)_sum': 'Precipitation_Sum',\n",
    "    'Snowfall (mm)_sum': 'Snowfall_Sum',\n",
    "    'Snow Cover (%)_mean': 'SnowCover_Mean'\n",
    "}\n",
    "\n",
    "merged_daily_df.rename(columns=new_names, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7130bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_daily_df.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad90407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check min/max date\n",
    "print(\"Dataset covers:\", merged_daily_df.index.min(), \"to\", merged_daily_df.index.max())\n",
    "\n",
    "# Create full expected daily date range\n",
    "full_range = pd.date_range(start=\"2023-06-03\", end=\"2024-09-23\", freq=\"D\")\n",
    "\n",
    "# Compare with your actual dataset index\n",
    "missing_dates = full_range.difference(merged_daily_df.index)\n",
    "\n",
    "print(\"Number of missing days:\", len(missing_dates))\n",
    "print(\"Missing days:\", missing_dates[:20])  # show first 20 if many"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2463efbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through all consumers\n",
    "sources = merged_daily_df['Source'].unique()\n",
    "for source in sources:\n",
    "    df_source = merged_daily_df[merged_daily_df['Source'] == source].copy()\n",
    "    df_source = df_source.sort_index()\n",
    "    \n",
    "    # Create complete daily index\n",
    "    full_idx = pd.date_range(df_source.index.min(), df_source.index.max(), freq='D')\n",
    "    df_source = df_source.reindex(full_idx)\n",
    "    \n",
    "    # Count missing days\n",
    "    missing_days = df_source['kwh'].isna().sum()\n",
    "    if missing_days > 0:\n",
    "        print(f\"Consumer {source} has {missing_days} missing days.\")\n",
    "    else:\n",
    "        print(f\"Consumer {source} has no missing days.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01857c5d",
   "metadata": {},
   "source": [
    "## **Prophet Model Testing**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da957408",
   "metadata": {},
   "source": [
    "**First Prophet Model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ac9dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = merged_daily_df.copy()\n",
    "df = df.sort_values('Date')\n",
    "\n",
    "target = 'kwh'\n",
    "climate_features = [\n",
    "       'Temp_Mean', 'Temp_Min',\n",
    "       'Temp_Max', 'Dewpoint_Mean',\n",
    "       'Dewpoint_Min', 'Dewpoint_Max',\n",
    "       'U_Wind_Mean', 'V_Wind_Mean',\n",
    "       'Precipitation_Sum', 'Snowfall_Sum',\n",
    "       'SnowCover_Mean'\n",
    "]\n",
    "\n",
    "# Lag and rolling features\n",
    "df['kwh_lag1'] = df[target].shift(1)\n",
    "df['kwh_lag7'] = df[target].shift(7)\n",
    "df['kwh_roll7'] = df[target].rolling(7).mean()\n",
    "df = df.dropna()\n",
    "\n",
    "\n",
    "# Drop NaN only for training/testing\n",
    "df_train_test = df.dropna().reset_index(drop=True)\n",
    "\n",
    "# Prophet format\n",
    "prophet_df = df_train_test.rename(columns={'Date':'ds', target:'y'})\n",
    "\n",
    "# Train-Test split\n",
    "train = prophet_df[prophet_df['ds'] <= '2024-08-23']\n",
    "test  = prophet_df[(prophet_df['ds'] >= '2024-08-24') & (prophet_df['ds'] <= '2024-09-23')]\n",
    "\n",
    "# Prophet model\n",
    "m = Prophet()\n",
    "\n",
    "# Add regressors\n",
    "for col in climate_features + ['kwh_lag1','kwh_lag7','kwh_roll7']:\n",
    "    m.add_regressor(col)\n",
    "\n",
    "# Fit\n",
    "m.fit(train[['ds','y'] + climate_features + ['kwh_lag1','kwh_lag7','kwh_roll7']])\n",
    "#save train features\n",
    "train_features = train[['ds','y'] + climate_features + ['kwh_lag1','kwh_lag7','kwh_roll7']]\n",
    "train_features.to_csv('models/prophet_train_features.csv', index=False)\n",
    "\n",
    "#save the model\n",
    "import pickle\n",
    "with open('models/prophet_model.pkl', 'wb') as f:\n",
    "    pickle.dump(m, f)\n",
    "\n",
    "# Predict test\n",
    "forecast_test = m.predict(test[['ds'] + climate_features + ['kwh_lag1','kwh_lag7','kwh_roll7']])\n",
    "#save test features\n",
    "test_features = test[['ds','y'] + climate_features + ['kwh_lag1','kwh_lag7','kwh_roll7']]\n",
    "test_features.to_csv('models/prophet_test_features.csv', index=False)\n",
    "\n",
    "rmse = sqrt(mean_squared_error(test['y'], forecast_test['yhat']))\n",
    "print(f\"Test RMSE: {rmse:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f7d5f3",
   "metadata": {},
   "source": [
    "**Prophet Model with higher RMSE.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25655be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = merged_daily_df.copy()\n",
    "df = df.sort_values('Date')\n",
    "\n",
    "target = 'kwh'\n",
    "climate_features = [\n",
    "    'Temp_Mean', 'Temp_Min', 'Temp_Max',\n",
    "    'Dewpoint_Mean', 'Dewpoint_Min', 'Dewpoint_Max',\n",
    "    'U_Wind_Mean', 'V_Wind_Mean',\n",
    "    'Precipitation_Sum', 'Snowfall_Sum', 'SnowCover_Mean'\n",
    "]\n",
    "\n",
    "# --- Lag features ---\n",
    "df['kwh_lag1'] = df.groupby('Source')[target].shift(1)\n",
    "df['kwh_lag7'] = df.groupby('Source')[target].shift(7)\n",
    "df['kwh_roll7'] = df.groupby('Source')[target].shift(1).rolling(7).mean()\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "# --- Prophet format ---\n",
    "prophet_df = df.rename(columns={'Date':'ds', target:'y'})\n",
    "\n",
    "# --- Train/Test split ---\n",
    "train = prophet_df[prophet_df['ds'] <= '2024-08-23']\n",
    "test  = prophet_df[(prophet_df['ds'] >= '2024-08-24') & (prophet_df['ds'] <= '2024-09-23')]\n",
    "\n",
    "# --- Train Prophet model ---\n",
    "m = Prophet()\n",
    "for col in climate_features + ['kwh_lag1','kwh_lag7','kwh_roll7']:\n",
    "    m.add_regressor(col)\n",
    "\n",
    "m.fit(train[['ds','y'] + climate_features + ['kwh_lag1','kwh_lag7','kwh_roll7']])\n",
    "\n",
    "#save train features\n",
    "train_features = train[['ds','y'] + climate_features + ['kwh_lag1','kwh_lag7','kwh_roll7']]\n",
    "train_features.to_csv('models/prophet_train_features.csv', index=False)\n",
    "\n",
    "#save the model\n",
    "import pickle\n",
    "with open('models/prophet_model.pkl', 'wb') as f:\n",
    "    pickle.dump(m, f)\n",
    "\n",
    "# --- Predict on test ---\n",
    "forecast_test = m.predict(test[['ds'] + climate_features + ['kwh_lag1','kwh_lag7','kwh_roll7']])\n",
    "\n",
    "#save test features\n",
    "test_features = test[['ds','y'] + climate_features + ['kwh_lag1','kwh_lag7','kwh_roll7']]\n",
    "test_features.to_csv('models/prophet_test_features.csv', index=False)\n",
    "\n",
    "rmse = sqrt(mean_squared_error(test['y'], forecast_test['yhat']))\n",
    "print(f\"Test RMSE: {rmse:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf5cf72",
   "metadata": {},
   "source": [
    "**Prediction for the extra month. Creation of Submission file for Zindi.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1251911b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Extra month forecast per source ---\n",
    "extra_month_df['ds'] = pd.to_datetime(extra_month_df['Date'])\n",
    "all_sources = merged_daily_df['Source'].unique()\n",
    "submission_rows = []\n",
    "\n",
    "# Initialize a DataFrame for daily total predictions\n",
    "daily_total = pd.DataFrame({'ds': extra_month_df['ds'].unique()})\n",
    "daily_total['pred_total_kwh'] = 0\n",
    "\n",
    "for source in all_sources:\n",
    "    # Historical data per source\n",
    "    df_source = merged_daily_df[merged_daily_df['Source']==source].sort_values('Date').reset_index(drop=True)\n",
    "    \n",
    "    # Lag history (last 30 days)\n",
    "    lag_hist = df_source.tail(30).copy().reset_index(drop=True)\n",
    "    \n",
    "    # Predict extra month recursively\n",
    "    for ds_date in extra_month_df['ds']:\n",
    "        row = extra_month_df[extra_month_df['ds'] == ds_date].iloc[0].copy()\n",
    "        row['kwh_lag1'] = lag_hist[target].iloc[-1]\n",
    "        row['kwh_lag7'] = lag_hist[target].iloc[-7] if len(lag_hist) >= 7 else lag_hist[target].mean()\n",
    "        row['kwh_roll7'] = lag_hist[target].iloc[-7:].mean() if len(lag_hist) >= 7 else lag_hist[target].mean()\n",
    "\n",
    "         # Fill missing climate features\n",
    "        for col in climate_features:\n",
    "            if pd.isna(row[col]):\n",
    "                row[col] = 0\n",
    "\n",
    "        predictors = ['ds'] + climate_features + ['kwh_lag1','kwh_lag7','kwh_roll7']\n",
    "        pred_kwh = m.predict(pd.DataFrame([row])[predictors])['yhat'].values[0]\n",
    "        \n",
    "        # Append submission row\n",
    "        submission_rows.append({\n",
    "            'ID': f\"{ds_date.date()}_{source}\",\n",
    "            'kwh': round(pred_kwh, 2)\n",
    "        })\n",
    "        \n",
    "        # Update lag history for next day prediction\n",
    "        lag_hist = pd.concat([lag_hist, pd.DataFrame([{target: pred_kwh}])], ignore_index=True)\n",
    "        \n",
    "        # Update daily total\n",
    "        daily_total.loc[daily_total['ds'] == ds_date, 'pred_total_kwh'] += pred_kwh\n",
    "\n",
    "# --- Create submission DataFrame ---\n",
    "submission_df = pd.DataFrame(submission_rows)\n",
    "submission_df.to_csv('prophet_submission_all_sources.csv', index=False)\n",
    "print(\"Submission file saved as prophet_submission_all_sources.csv\")\n",
    "\n",
    "# --- Plot ---\n",
    "plt.figure(figsize=(14,6))\n",
    "\n",
    "# Historical actual total per day\n",
    "historical_total = merged_daily_df.groupby('Date')[target].sum().reset_index()\n",
    "plt.plot(historical_total['Date'], historical_total[target], label='Actual Total (Historical)', color='blue')\n",
    "\n",
    "# Extra month predicted total\n",
    "plt.plot(daily_total['ds'], daily_total['pred_total_kwh'], label='Predicted Total (Extra Month)', color='orange')\n",
    "\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Daily Total kWh')\n",
    "plt.title('Prophet Forecast: Total Daily kWh for All Sources (Extra Month)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
