{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066c1f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcf0c3d",
   "metadata": {},
   "source": [
    "-------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72681427",
   "metadata": {},
   "source": [
    "# Prepare data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2392e481",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Load the climate data\n",
    "\n",
    "file_path = \"Kalam Climate Data.xlsx\"   \n",
    "climate_df = pd.read_excel(file_path)\n",
    "\n",
    "climate_df['Date Time'] = pd.to_datetime(climate_df['Date Time'])\n",
    "\n",
    "# Extract date\n",
    "\n",
    "climate_df['date'] = climate_df['Date Time'].dt.date\n",
    "\n",
    "# Daily aggregation\n",
    "daily_climate = climate_df.groupby('date').agg({\n",
    "    'Temperature (Â°C)': ['mean', 'min', 'max'],\n",
    "    'Dewpoint Temperature (Â°C)': ['mean', 'min', 'max'],\n",
    "    'U Wind Component (m/s)': 'mean',\n",
    "    'V Wind Component (m/s)': 'mean',\n",
    "    'Total Precipitation (mm)': 'sum',\n",
    "    'Snowfall (mm)': 'sum',\n",
    "    'Snow Cover (%)': 'mean'\n",
    "})\n",
    "\n",
    "# Flatten columns\n",
    "daily_climate.columns = ['_'.join(col).strip() for col in daily_climate.columns.values]\n",
    "\n",
    "# Reset index\n",
    "daily_climate = daily_climate.reset_index()\n",
    "\n",
    "# Define start and end dates\n",
    "start_date = pd.to_datetime('2024-09-24')\n",
    "end_date   = pd.to_datetime('2024-10-24')\n",
    "\n",
    "daily_climate[\"date\"] = pd.to_datetime(daily_climate[\"date\"])\n",
    "\n",
    "# Filter DataFrame\n",
    "extra_month_df = daily_climate[(daily_climate['date'] >= start_date) & (daily_climate['date'] <= end_date)]\n",
    "\n",
    "\n",
    "new_names = {\n",
    "    'date': 'Date',\n",
    "    'Temperature (Â°C)_mean': 'Temp_Mean',\n",
    "    'Temperature (Â°C)_min': 'Temp_Min',\n",
    "    'Temperature (Â°C)_max': 'Temp_Max',\n",
    "    'Dewpoint Temperature (Â°C)_mean': 'Dewpoint_Mean',\n",
    "    'Dewpoint Temperature (Â°C)_min': 'Dewpoint_Min',\n",
    "    'Dewpoint Temperature (Â°C)_max': 'Dewpoint_Max',\n",
    "    'U Wind Component (m/s)_mean': 'U_Wind_Mean',\n",
    "    'V Wind Component (m/s)_mean': 'V_Wind_Mean',\n",
    "    'Total Precipitation (mm)_sum': 'Precipitation_Sum',\n",
    "    'Snowfall (mm)_sum': 'Snowfall_Sum',\n",
    "    'Snow Cover (%)_mean': 'SnowCover_Mean'\n",
    "}\n",
    "\n",
    "\n",
    "extra_month_df.rename(columns=new_names, inplace=True)\n",
    "\n",
    "all_data_df = pd.read_csv(\"Data.csv\")\n",
    "\n",
    "all_data_df.drop(columns=[\"consumer_device_9\", \"consumer_device_x\", \"v_red\", \"v_blue\",\"v_yellow\", \"current\", \"power_factor\"], inplace=True)\n",
    "all_data_df.head()\n",
    "\n",
    "# Ensure datetime\n",
    "all_data_df['date_time'] = pd.to_datetime(all_data_df['date_time'])\n",
    "\n",
    "\n",
    "# Extract date (drop time)\n",
    "all_data_df['date'] = all_data_df['date_time'].dt.date\n",
    "\n",
    "\n",
    "# Group by Source (consumer_device + data_user) and date\n",
    "\n",
    "daily_df = all_data_df.groupby(['Source', 'date']).agg({\n",
    "    'kwh': 'sum'  \n",
    "})\n",
    "\n",
    "daily_df = daily_df.reset_index()\n",
    "\n",
    "\n",
    "# Ensure datetime index\n",
    "daily_df = daily_df.set_index(\"date\").sort_index()\n",
    "daily_climate = daily_climate.set_index(\"date\").sort_index()\n",
    "\n",
    "# Merge\n",
    "merged_daily_df = daily_df.join(daily_climate, how=\"left\")\n",
    "\n",
    "\n",
    "merged_daily_df.reset_index(inplace=True)\n",
    "\n",
    "merged_daily_df.to_csv(\"second_daily_merged_hydro_climate.csv\", index=False)\n",
    "\n",
    "\n",
    "# Dictionary mapping old names to new names\n",
    "new_names = {\n",
    "    'date': 'Date',\n",
    "    'Source': 'Source',\n",
    "    'kwh': 'kwh',\n",
    "    'Temperature (Â°C)_mean': 'Temp_Mean',\n",
    "    'Temperature (Â°C)_min': 'Temp_Min',\n",
    "    'Temperature (Â°C)_max': 'Temp_Max',\n",
    "    'Dewpoint Temperature (Â°C)_mean': 'Dewpoint_Mean',\n",
    "    'Dewpoint Temperature (Â°C)_min': 'Dewpoint_Min',\n",
    "    'Dewpoint Temperature (Â°C)_max': 'Dewpoint_Max',\n",
    "    'U Wind Component (m/s)_mean': 'U_Wind_Mean',\n",
    "    'V Wind Component (m/s)_mean': 'V_Wind_Mean',\n",
    "    'Total Precipitation (mm)_sum': 'Precipitation_Sum',\n",
    "    'Snowfall (mm)_sum': 'Snowfall_Sum',\n",
    "    'Snow Cover (%)_mean': 'SnowCover_Mean'\n",
    "}\n",
    "\n",
    "merged_daily_df.rename(columns=new_names, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18bce919",
   "metadata": {},
   "source": [
    "----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bc22c1",
   "metadata": {},
   "source": [
    "# Let us train a Feed-Forward Neural Network (MLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64c9aa0",
   "metadata": {},
   "source": [
    "# 1 st Approach: -----> Zindi Score 4.928191196 kWh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9acd9c4",
   "metadata": {},
   "source": [
    "Feature Engineering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2dce658",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features(df):\n",
    "    df = df.copy()\n",
    "    df[\"Temp_dew_diff\"] = df[\"Temp_Mean\"] - df[\"Dewpoint_Mean\"]\n",
    "    df[\"wind_speed\"] = np.sqrt(df[\"U_Wind_Mean\"]**2 + df[\"V_Wind_Mean\"]**2)\n",
    "    df[\"precip_snow_ratio\"] = df[\"Precipitation_Sum\"] / (df[\"Snowfall_Sum\"] + 1e-6)\n",
    "    return df\n",
    "\n",
    "def add_lag_roll(df, group_col=\"Source\", target_col=\"kwh\", lags=[1,2,7], windows=[3,7]):\n",
    "    df = df.sort_values([\"Source\",\"Date\"]).copy()\n",
    "    for lag in lags:\n",
    "        df[f\"lag_{lag}\"] = df.groupby(group_col)[target_col].shift(lag)\n",
    "    for w in windows:\n",
    "        df[f\"roll_mean_{w}\"] = df.groupby(group_col)[target_col].shift(1).rolling(w).mean()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c19c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Reindex all Sources across full date range ---\n",
    "def reindex_sources(df):\n",
    "    all_dates = pd.date_range(df[\"Date\"].min(), df[\"Date\"].max())\n",
    "    all_sources = df[\"Source\"].unique()\n",
    "    idx = pd.MultiIndex.from_product([all_sources, all_dates], names=[\"Source\",\"Date\"])\n",
    "    df_full = df.set_index([\"Source\",\"Date\"]).reindex(idx).reset_index()\n",
    "    df_full[\"kwh\"] = df_full[\"kwh\"].fillna(0)\n",
    "    climate_cols = [c for c in df.columns if c not in [\"Source\",\"kwh\",\"Date\"]]\n",
    "    for c in climate_cols:\n",
    "        df_full[c] = df_full.groupby(\"Date\")[c].transform(\"first\")\n",
    "    return df_full\n",
    "\n",
    "\n",
    "# --- Prepare dataset ---\n",
    "df = merged_daily_df.copy()\n",
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "df = reindex_sources(df)\n",
    "df = add_features(df)\n",
    "df = add_lag_roll(df)\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "feature_cols = [\n",
    "    \"Temp_Mean\",\"Temp_Min\",\"Temp_Max\",\"Dewpoint_Mean\",\"Dewpoint_Min\",\"Dewpoint_Max\",\n",
    "    \"U_Wind_Mean\",\"V_Wind_Mean\",\"Precipitation_Sum\",\"Snowfall_Sum\",\"SnowCover_Mean\",\n",
    "    \"Temp_dew_diff\",\"wind_speed\",\"precip_snow_ratio\",\n",
    "] + [c for c in df.columns if c.startswith(\"lag_\") or c.startswith(\"roll_\")]\n",
    "\n",
    "X = df[feature_cols].values\n",
    "y = df[\"kwh\"].values\n",
    "\n",
    "\n",
    "\n",
    "# Scale features for neural net\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff603fb",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b6462f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# --- Build MLP model ---\n",
    "def create_mlp(input_dim):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(128, activation=\"relu\", input_dim=input_dim))\n",
    "    model.add(layers.Dense(64, activation=\"relu\"))\n",
    "    model.add(layers.Dense(32, activation=\"relu\"))\n",
    "    model.add(layers.Dense(1, activation=\"relu\"))  # relu ensures no negative kwh\n",
    "    model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
    "    return model\n",
    "\n",
    "mlp = create_mlp(X_train.shape[1])\n",
    "\n",
    "# --- Callbacks: EarlyStopping + ReduceLROnPlateau ---\n",
    "early_stop = callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\", patience=5, restore_best_weights=True, verbose=0\n",
    ")\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\", factor=0.5, patience=3, min_lr=1e-6, verbose=0\n",
    ")\n",
    "\n",
    "history = mlp.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test,y_test),\n",
    "    epochs=100, batch_size=256, verbose=0,\n",
    "    callbacks=[early_stop, reduce_lr]\n",
    ")\n",
    "\n",
    "# Predict on test\n",
    "y_pred = mlp.predict(X_test, verbose=0).flatten()\n",
    "y_pred = np.maximum(0, y_pred)  # safety clip\n",
    "\n",
    "# --- Global Metrics ---\n",
    "print(\"=== Global Metrics ===\")\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "print(f\"Regression: RMSE={rmse:.3f}, MAPE={mape:.3f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5447819",
   "metadata": {},
   "source": [
    "# Predict next month (extra climate data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a17a269",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "extra = add_features(extra_month_df.copy())\n",
    "extra[\"Date\"] = pd.to_datetime(extra[\"Date\"])\n",
    "\n",
    "preds = []\n",
    "sources = df[\"Source\"].unique()\n",
    "for src in sources:\n",
    "    hist = df[df[\"Source\"]==src].copy()\n",
    "    extra_src = extra.copy()\n",
    "    extra_src[\"Source\"] = src\n",
    "    lag_hist = hist.tail(7).reset_index(drop=True)\n",
    "    rows = []\n",
    "    for _, row in extra_src.iterrows():\n",
    "        r = row.to_dict()\n",
    "        for lag in [1,2,7]:\n",
    "            if len(lag_hist) >= lag:\n",
    "                r[f\"lag_{lag}\"] = lag_hist.loc[len(lag_hist)-lag,\"kwh\"]\n",
    "            else:\n",
    "                r[f\"lag_{lag}\"] = np.nan\n",
    "        for w in [3,7]:\n",
    "            if len(lag_hist) >= w:\n",
    "                r[f\"roll_mean_{w}\"] = lag_hist[\"kwh\"].iloc[-w:].mean()\n",
    "            else:\n",
    "                r[f\"roll_mean_{w}\"] = lag_hist[\"kwh\"].mean()\n",
    "        X_ex = pd.DataFrame([r])[feature_cols].fillna(0)\n",
    "        X_ex = scaler.transform(X_ex)\n",
    "        r[\"pred_kwh\"] = max(0, mlp.predict(X_ex, verbose=0)[0,0])\n",
    "        lag_hist = pd.concat([lag_hist, pd.DataFrame([{\"kwh\": r[\"pred_kwh\"]}])], ignore_index=True)\n",
    "        rows.append(r)\n",
    "    preds.append(pd.DataFrame(rows))\n",
    "preds_df = pd.concat(preds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8c3bed",
   "metadata": {},
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44eaff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Per Source\n",
    "\n",
    "def plot_source(src):\n",
    "    plt.figure(figsize=(12,4))\n",
    "    d = df[df[\"Source\"]==src]\n",
    "    plt.plot(d[\"Date\"], d[\"kwh\"], label=\"actual\")\n",
    "    d_extra = preds_df[preds_df[\"Source\"]==src]\n",
    "    plt.plot(d_extra[\"Date\"], d_extra[\"pred_kwh\"], \"--\", label=\"pred (extra month)\")\n",
    "    plt.title(f\"Source {src}\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "for src in sources:\n",
    "    plot_source(src)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4535976a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall \n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(df.groupby(\"Date\")[\"kwh\"].sum(), label=\"actual total\")\n",
    "plt.plot(preds_df.groupby(\"Date\")[\"pred_kwh\"].sum(), \"--\", label=\"pred total (extra month)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d58cbc",
   "metadata": {},
   "source": [
    "# Build Submission file for Zindi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04318d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv(\"SampleSubmission.csv\")\n",
    "sub[\"Date\"] = sub[\"ID\"].apply(lambda x: x.split(\"_\")[0])\n",
    "sub[\"Source\"] = sub[\"ID\"].apply(lambda x: \"_\".join(x.split(\"_\")[1:]))\n",
    "sub[\"Date\"] = pd.to_datetime(sub[\"Date\"])\n",
    "\n",
    "preds_out = preds_df.copy()\n",
    "preds_out[\"Date\"] = pd.to_datetime(preds_out[\"Date\"])\n",
    "\n",
    "submission = sub.merge(preds_out[[\"Date\",\"Source\",\"pred_kwh\"]], on=[\"Date\",\"Source\"], how=\"left\")\n",
    "submission = submission[[\"ID\",\"pred_kwh\"]]\n",
    "\n",
    "submission.to_csv(\"MySubmission_3_MLP.csv\", index=False)\n",
    "submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b330a150",
   "metadata": {},
   "source": [
    "-------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee9a1f1",
   "metadata": {},
   "source": [
    "# Second Approach:   performed worse ----> Zindi 6.431916951\n",
    "\n",
    "1) Add calendar features + longer rolling windows:\n",
    "\n",
    "- Calendar features (day_of_week, month, is_weekend, season)\n",
    "\n",
    "- Longer rolling windows (14 and 30 days)\n",
    "\n",
    "- Exponential moving average (EMA) features\n",
    "\n",
    "2) Added Dropout (0.2) layers for better generalization.\n",
    "\n",
    "3) Extended lag history to 30 days when predicting extra month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d4e06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Feature Engineering \n",
    "\n",
    "\n",
    "def add_features(df):\n",
    "    df = df.copy()\n",
    "    # Climate derived features\n",
    "    df[\"Temp_dew_diff\"] = df[\"Temp_Mean\"] - df[\"Dewpoint_Mean\"]\n",
    "    df[\"wind_speed\"] = np.sqrt(df[\"U_Wind_Mean\"]**2 + df[\"V_Wind_Mean\"]**2)\n",
    "    df[\"precip_snow_ratio\"] = df[\"Precipitation_Sum\"] / (df[\"Snowfall_Sum\"] + 1e-6)\n",
    "    \n",
    "    # Calendar features\n",
    "    df[\"day_of_week\"] = df[\"Date\"].dt.dayofweek\n",
    "    df[\"month\"] = df[\"Date\"].dt.month\n",
    "    df[\"is_weekend\"] = (df[\"day_of_week\"] >= 5).astype(int)\n",
    "    # Simple season encoding (1â€“4)\n",
    "    df[\"season\"] = ((df[\"month\"] % 12 + 3) // 3)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def add_lag_roll(df, group_col=\"Source\", target_col=\"kwh\", lags=[1,2,7], windows=[3,7,14,30]):\n",
    "    df = df.sort_values([group_col,\"Date\"]).copy()\n",
    "    # Lags\n",
    "    for lag in lags:\n",
    "        df[f\"lag_{lag}\"] = df.groupby(group_col)[target_col].shift(lag)\n",
    "    # Rolling means\n",
    "    for w in windows:\n",
    "        df[f\"roll_mean_{w}\"] = df.groupby(group_col)[target_col].shift(1).rolling(w).mean()\n",
    "    # Exponential moving averages\n",
    "    for span in [7,14,30]:\n",
    "        df[f\"ema_{span}\"] = df.groupby(group_col)[target_col].shift(1).ewm(span=span, adjust=False).mean()\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_source_stats(df, group_col=\"Source\", target_col=\"kwh\"):\n",
    "    stats = df.groupby(group_col)[target_col].agg([\"mean\",\"median\"]).reset_index()\n",
    "    stats.columns = [group_col, \"src_mean_kwh\", \"src_median_kwh\"]\n",
    "    df = df.merge(stats, on=group_col, how=\"left\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def reindex_sources(df):\n",
    "    all_dates = pd.date_range(df[\"Date\"].min(), df[\"Date\"].max())\n",
    "    all_sources = df[\"Source\"].unique()\n",
    "    idx = pd.MultiIndex.from_product([all_sources, all_dates], names=[\"Source\",\"Date\"])\n",
    "    df_full = df.set_index([\"Source\",\"Date\"]).reindex(idx).reset_index()\n",
    "    df_full[\"kwh\"] = df_full[\"kwh\"].fillna(0)\n",
    "    climate_cols = [c for c in df.columns if c not in [\"Source\",\"kwh\",\"Date\"]]\n",
    "    for c in climate_cols:\n",
    "        df_full[c] = df_full.groupby(\"Date\")[c].transform(\"first\")\n",
    "    return df_full\n",
    "\n",
    "\n",
    "\n",
    "#  Prepare dataset \n",
    "\n",
    "\n",
    "df = merged_daily_df.copy()\n",
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "df = reindex_sources(df)\n",
    "df = add_features(df)\n",
    "df = add_lag_roll(df)\n",
    "df = add_source_stats(df)\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "# Features\n",
    "feature_cols = [\n",
    "    \"Temp_Mean\",\"Temp_Min\",\"Temp_Max\",\"Dewpoint_Mean\",\"Dewpoint_Min\",\"Dewpoint_Max\",\n",
    "    \"U_Wind_Mean\",\"V_Wind_Mean\",\"Precipitation_Sum\",\"Snowfall_Sum\",\"SnowCover_Mean\",\n",
    "    \"Temp_dew_diff\",\"wind_speed\",\"precip_snow_ratio\",\n",
    "    \"day_of_week\",\"month\",\"is_weekend\",\"season\",\n",
    "    \"src_mean_kwh\",\"src_median_kwh\"\n",
    "] + [c for c in df.columns if c.startswith(\"lag_\") or c.startswith(\"roll_\") or c.startswith(\"ema_\")]\n",
    "\n",
    "X = df[feature_cols].values\n",
    "y = df[\"kwh\"].values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "#  Build MLP model \n",
    "\n",
    "\n",
    "def create_mlp(input_dim):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(128, activation=\"relu\", input_dim=input_dim))\n",
    "    model.add(layers.Dropout(0.2))\n",
    "    model.add(layers.Dense(64, activation=\"relu\"))\n",
    "    model.add(layers.Dropout(0.2))\n",
    "    model.add(layers.Dense(32, activation=\"relu\"))\n",
    "    model.add(layers.Dense(1, activation=\"relu\"))                                                         # Here relu ensures no negative predictions, NO NEED TO WORRY ABOUT IT !\n",
    "    model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
    "    return model\n",
    "\n",
    "mlp = create_mlp(X_train.shape[1])\n",
    "\n",
    "early_stop = callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\", patience=5, restore_best_weights=True, verbose=0\n",
    ")\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\", factor=0.5, patience=3, min_lr=1e-6, verbose=0\n",
    ")\n",
    "\n",
    "history = mlp.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=100, batch_size=256, verbose=0,\n",
    "    callbacks=[early_stop, reduce_lr]\n",
    ")\n",
    "\n",
    "\n",
    "#  Evaluate on test set \n",
    "\n",
    "y_pred = mlp.predict(X_test, verbose=0).flatten()\n",
    "y_pred = np.maximum(0, y_pred)\n",
    "\n",
    "print(\"=== Global Metrics ===\")\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "print(f\"Regression: RMSE={rmse:.3f}, MAPE={mape:.3f}\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e330c2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Predict extra month \n",
    "\n",
    "extra = add_features(extra_month_df.copy())\n",
    "extra[\"Date\"] = pd.to_datetime(extra[\"Date\"])\n",
    "\n",
    "preds = []\n",
    "sources = df[\"Source\"].unique()\n",
    "for src in sources:\n",
    "    hist = df[df[\"Source\"]==src].copy()\n",
    "    extra_src = extra.copy()\n",
    "    extra_src[\"Source\"] = src\n",
    "    lag_hist = hist.tail(30).reset_index(drop=True)  # keep last 30 days\n",
    "    rows = []\n",
    "    for _, row in extra_src.iterrows():\n",
    "        r = row.to_dict()\n",
    "        # Lags\n",
    "        for lag in [1,2,7]:\n",
    "            if len(lag_hist) >= lag:\n",
    "                r[f\"lag_{lag}\"] = lag_hist.loc[len(lag_hist)-lag, \"kwh\"]\n",
    "            else:\n",
    "                r[f\"lag_{lag}\"] = 0\n",
    "        # Rolling means\n",
    "        for w in [3,7,14,30]:\n",
    "            if len(lag_hist) >= w:\n",
    "                r[f\"roll_mean_{w}\"] = lag_hist[\"kwh\"].iloc[-w:].mean()\n",
    "            else:\n",
    "                r[f\"roll_mean_{w}\"] = lag_hist[\"kwh\"].mean()\n",
    "        # EMA\n",
    "        for span in [7,14,30]:\n",
    "            r[f\"ema_{span}\"] = lag_hist[\"kwh\"].ewm(span=span, adjust=False).mean().iloc[-1]\n",
    "        # Source stats\n",
    "        r[\"src_mean_kwh\"] = hist[\"kwh\"].mean()\n",
    "        r[\"src_median_kwh\"] = hist[\"kwh\"].median()\n",
    "        # Predict\n",
    "        X_ex = pd.DataFrame([r])[feature_cols].fillna(0)\n",
    "        X_ex = scaler.transform(X_ex)\n",
    "        r[\"pred_kwh\"] = max(0, mlp.predict(X_ex, verbose=0)[0,0])\n",
    "        lag_hist = pd.concat([lag_hist, pd.DataFrame([{\"kwh\": r[\"pred_kwh\"]}])], ignore_index=True)\n",
    "        rows.append(r)\n",
    "    preds.append(pd.DataFrame(rows))\n",
    "preds_df = pd.concat(preds)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f1d863",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Prepare submission \n",
    "\n",
    "sub = pd.read_csv(\"SampleSubmission.csv\")\n",
    "sub[\"Date\"] = sub[\"ID\"].apply(lambda x: x.split(\"_\")[0])\n",
    "sub[\"Source\"] = sub[\"ID\"].apply(lambda x: \"_\".join(x.split(\"_\")[1:]))\n",
    "sub[\"Date\"] = pd.to_datetime(sub[\"Date\"])\n",
    "\n",
    "preds_out = preds_df.copy()\n",
    "preds_out[\"Date\"] = pd.to_datetime(preds_out[\"Date\"])\n",
    "\n",
    "submission = sub.merge(preds_out[[\"Date\",\"Source\",\"pred_kwh\"]], on=[\"Date\",\"Source\"], how=\"left\")\n",
    "submission = submission[[\"ID\",\"pred_kwh\"]]\n",
    "submission.to_csv(\"MySubmission_5_MLP.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59244807",
   "metadata": {},
   "source": [
    "-------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b049ea2",
   "metadata": {},
   "source": [
    "# Third approach -----> zindi 4.50048738  (4th Position !!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6eae425",
   "metadata": {},
   "source": [
    "Let us not apply all the changes together, but one by one. Let us try first only adding the drop out layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9bec99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering (unchanged)\n",
    "\n",
    "def add_features(df):\n",
    "    df = df.copy()\n",
    "    df[\"Temp_dew_diff\"] = df[\"Temp_Mean\"] - df[\"Dewpoint_Mean\"]\n",
    "    df[\"wind_speed\"] = np.sqrt(df[\"U_Wind_Mean\"]**2 + df[\"V_Wind_Mean\"]**2)\n",
    "    df[\"precip_snow_ratio\"] = df[\"Precipitation_Sum\"] / (df[\"Snowfall_Sum\"] + 1e-6)\n",
    "    return df\n",
    "\n",
    "def add_lag_roll(df, group_col=\"Source\", target_col=\"kwh\", lags=[1,2,7], windows=[3,7]):\n",
    "    df = df.sort_values([group_col,\"Date\"]).copy()\n",
    "    for lag in lags:\n",
    "        df[f\"lag_{lag}\"] = df.groupby(group_col)[target_col].shift(lag)\n",
    "    for w in windows:\n",
    "        df[f\"roll_mean_{w}\"] = df.groupby(group_col)[target_col].shift(1).rolling(w).mean()\n",
    "    return df\n",
    "\n",
    "def reindex_sources(df):\n",
    "    all_dates = pd.date_range(df[\"Date\"].min(), df[\"Date\"].max())\n",
    "    all_sources = df[\"Source\"].unique()\n",
    "    idx = pd.MultiIndex.from_product([all_sources, all_dates], names=[\"Source\",\"Date\"])\n",
    "    df_full = df.set_index([\"Source\",\"Date\"]).reindex(idx).reset_index()\n",
    "    df_full[\"kwh\"] = df_full[\"kwh\"].fillna(0)\n",
    "    climate_cols = [c for c in df.columns if c not in [\"Source\",\"kwh\",\"Date\"]]\n",
    "    for c in climate_cols:\n",
    "        df_full[c] = df_full.groupby(\"Date\")[c].transform(\"first\")\n",
    "    return df_full\n",
    "\n",
    "\n",
    "# Prepare dataset\n",
    "\n",
    "df = merged_daily_df.copy()\n",
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "df = reindex_sources(df)                \n",
    "df = add_features(df)\n",
    "df = add_lag_roll(df)\n",
    "df = df.dropna().reset_index(drop=True) \n",
    "\n",
    "feature_cols = [\n",
    "    \"Temp_Mean\",\"Temp_Min\",\"Temp_Max\",\"Dewpoint_Mean\",\"Dewpoint_Min\",\"Dewpoint_Max\",\n",
    "    \"U_Wind_Mean\",\"V_Wind_Mean\",\"Precipitation_Sum\",\"Snowfall_Sum\",\"SnowCover_Mean\",\n",
    "    \"Temp_dew_diff\",\"wind_speed\",\"precip_snow_ratio\",\n",
    "] + [c for c in df.columns if c.startswith(\"lag_\") or c.startswith(\"roll_\")]\n",
    "\n",
    "# train/test split \n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "# scale features for MLP\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(train_df[feature_cols])\n",
    "y_train = train_df[\"kwh\"].values\n",
    "X_test = scaler.transform(test_df[feature_cols])\n",
    "y_test = test_df[\"kwh\"].values\n",
    "\n",
    "# -------------------------\n",
    "# MLP model (only addition: Dropout) !!\n",
    "# -------------------------\n",
    "def create_mlp_with_dropout(input_dim):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(128, activation=\"relu\", input_dim=input_dim))\n",
    "    model.add(layers.Dropout(0.2))            # <-- added dropout\n",
    "    model.add(layers.Dense(64, activation=\"relu\"))\n",
    "    model.add(layers.Dropout(0.2))            # <-- added dropout\n",
    "    model.add(layers.Dense(32, activation=\"relu\"))\n",
    "    model.add(layers.Dense(1, activation=\"relu\"))  # ReLU to avoid negative predictions\n",
    "    model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
    "    return model\n",
    "\n",
    "mlp = create_mlp_with_dropout(X_train.shape[1])\n",
    "\n",
    "early_stop = callbacks.EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True, verbose=0)\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=3, min_lr=1e-6, verbose=0)\n",
    "\n",
    "history = mlp.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=100, batch_size=256, verbose=0,\n",
    "    callbacks=[early_stop, reduce_lr]\n",
    ")\n",
    "\n",
    "\n",
    "# Evaluate on test set\n",
    "\n",
    "y_pred = mlp.predict(X_test, verbose=0).flatten()\n",
    "y_pred = np.maximum(0, y_pred)   \n",
    "\n",
    "print(\"=== Global Metrics ===\")\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "print(f\"Regression: RMSE={rmse:.4f}, MAPE={mape:.4f}\\n\")\n",
    "\n",
    "# per-source metrics on TEST (helpful to compare!)\n",
    "per_source = []\n",
    "for src in sorted(df[\"Source\"].unique()):\n",
    "    t_src = test_df[test_df[\"Source\"] == src]\n",
    "    if t_src.shape[0] == 0:\n",
    "        continue\n",
    "    # build X for this source in test_df order\n",
    "    X_src = scaler.transform(t_src[feature_cols])\n",
    "    y_src = t_src[\"kwh\"].values\n",
    "    y_src_pred = np.maximum(0, mlp.predict(X_src, verbose=0).flatten())\n",
    "    rm = mean_squared_error(y_src, y_src_pred, squared=False)\n",
    "    mp = mean_absolute_percentage_error(y_src, y_src_pred)\n",
    "    per_source.append((src, rm, mp, len(t_src)))\n",
    "per_source_df = pd.DataFrame(per_source, columns=[\"Source\",\"RMSE\",\"MAPE\",\"N_test_rows\"])\n",
    "print(\"Per-source TEST sample (first rows):\")\n",
    "print(per_source_df.head(10).to_string(index=False))\n",
    "\n",
    "# Let us see the training curves\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(history.history[\"loss\"], label=\"train loss\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"val loss\")\n",
    "plt.legend(); plt.title(\"Training Curves\"); plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bcf47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extra-month predictions \n",
    "\n",
    "extra = add_features(extra_month_df.copy())\n",
    "extra[\"Date\"] = pd.to_datetime(extra[\"Date\"])\n",
    "\n",
    "preds = []\n",
    "sources = df[\"Source\"].unique()\n",
    "for src in sources:\n",
    "    hist = df[df[\"Source\"]==src].copy()\n",
    "    extra_src = extra.copy()\n",
    "    extra_src[\"Source\"] = src\n",
    "    lag_hist = hist.tail(7).reset_index(drop=True)  # last 7 days to compute lags/rolls\n",
    "    rows = []\n",
    "    for _, row in extra_src.iterrows():\n",
    "        r = row.to_dict()\n",
    "        # lags\n",
    "        for lag in [1,2,7]:\n",
    "            if len(lag_hist) >= lag:\n",
    "                r[f\"lag_{lag}\"] = lag_hist.loc[len(lag_hist)-lag, \"kwh\"]\n",
    "            else:\n",
    "                r[f\"lag_{lag}\"] = 0\n",
    "        # rolling means\n",
    "        for w in [3,7]:\n",
    "            if len(lag_hist) >= w:\n",
    "                r[f\"roll_mean_{w}\"] = lag_hist[\"kwh\"].iloc[-w:].mean()\n",
    "            else:\n",
    "                r[f\"roll_mean_{w}\"] = lag_hist[\"kwh\"].mean() if len(lag_hist)>0 else 0\n",
    "        # predict\n",
    "        X_ex = pd.DataFrame([r])[feature_cols].fillna(0)\n",
    "        X_ex = scaler.transform(X_ex)\n",
    "        r[\"pred_kwh\"] = float(max(0, mlp.predict(X_ex, verbose=0)[0,0]))\n",
    "        # append to history so next day uses predicted values as lags\n",
    "        lag_hist = pd.concat([lag_hist, pd.DataFrame([{\"kwh\": r[\"pred_kwh\"]}])], ignore_index=True)\n",
    "        rows.append(r)\n",
    "    preds.append(pd.DataFrame(rows))\n",
    "preds_df = pd.concat(preds, ignore_index=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f34e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnostic plots (10 random sources + aggregated)\n",
    "\n",
    "random.seed(42)\n",
    "sample_sources = random.sample(list(df[\"Source\"].unique()), min(10, len(df[\"Source\"].unique())))\n",
    "\n",
    "for src in sample_sources:\n",
    "    g_true = df[df[\"Source\"] == src].sort_values(\"Date\")\n",
    "    g_pred = preds_df[preds_df[\"Source\"] == src].sort_values(\"Date\")\n",
    "    plt.figure(figsize=(10,3))\n",
    "    plt.plot(g_true[\"Date\"], g_true[\"kwh\"], label=\"history\", color=\"blue\")\n",
    "    plt.plot(g_pred[\"Date\"], g_pred[\"pred_kwh\"], \"--\", label=\"pred extra month\", color=\"red\")\n",
    "    plt.title(f\"Source: {src}\")\n",
    "    plt.ylabel(\"kWh\"); plt.legend(); plt.show()\n",
    "\n",
    "agg_true = df.groupby(\"Date\")[\"kwh\"].sum().reset_index()\n",
    "agg_pred = preds_df.groupby(\"Date\")[\"pred_kwh\"].sum().reset_index()\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(agg_true[\"Date\"], agg_true[\"kwh\"], label=\"total history\", color=\"blue\")\n",
    "plt.plot(agg_pred[\"Date\"], agg_pred[\"pred_kwh\"], \"--\", label=\"total predicted (extra month)\", color=\"red\")\n",
    "plt.legend(); plt.title(\"Aggregated consumption\"); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d0391c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare submission \n",
    "\n",
    "sub = pd.read_csv(\"SampleSubmission.csv\")\n",
    "sub[\"Date\"] = sub[\"ID\"].apply(lambda x: x.split(\"_\")[0])\n",
    "sub[\"Source\"] = sub[\"ID\"].apply(lambda x: \"_\".join(x.split(\"_\")[1:]))\n",
    "sub[\"Date\"] = pd.to_datetime(sub[\"Date\"])\n",
    "\n",
    "preds_out = preds_df.copy()\n",
    "preds_out[\"Date\"] = pd.to_datetime(preds_out[\"Date\"])\n",
    "\n",
    "submission = sub.merge(preds_out[[\"Date\",\"Source\",\"pred_kwh\"]], on=[\"Date\",\"Source\"], how=\"left\")\n",
    "submission = submission[[\"ID\",\"pred_kwh\"]]\n",
    "submission.to_csv(\"MySubmission_6_MLP.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7185fd74",
   "metadata": {},
   "source": [
    "--------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b1cf46",
   "metadata": {},
   "source": [
    "# Fourth Approach: -------> Zindi 4.48370432   (2nd Place)\n",
    "\n",
    "Letâ€™s add just one longer lag (lag_14) and one longer rolling window (roll_mean_14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3614da25",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# --- Feature Engineering ---\n",
    "def add_features(df):\n",
    "    df = df.copy()\n",
    "    df[\"Temp_dew_diff\"] = df[\"Temp_Mean\"] - df[\"Dewpoint_Mean\"]\n",
    "    df[\"wind_speed\"] = np.sqrt(df[\"U_Wind_Mean\"]**2 + df[\"V_Wind_Mean\"]**2)\n",
    "    df[\"precip_snow_ratio\"] = df[\"Precipitation_Sum\"] / (df[\"Snowfall_Sum\"] + 1e-6)\n",
    "    return df\n",
    "\n",
    "def add_lag_roll(df, group_col=\"Source\", target_col=\"kwh\", lags=[1,2,7,14], windows=[3,7,14]):\n",
    "    df = df.sort_values([\"Source\",\"Date\"]).copy()\n",
    "    for lag in lags:\n",
    "        df[f\"lag_{lag}\"] = df.groupby(group_col)[target_col].shift(lag)\n",
    "    for w in windows:\n",
    "        df[f\"roll_mean_{w}\"] = df.groupby(group_col)[target_col].shift(1).rolling(w).mean()\n",
    "    return df\n",
    "\n",
    "# --- Step 1: Reindex all Sources across full date range ---\n",
    "def reindex_sources(df):\n",
    "    all_dates = pd.date_range(df[\"Date\"].min(), df[\"Date\"].max())\n",
    "    all_sources = df[\"Source\"].unique()\n",
    "    idx = pd.MultiIndex.from_product([all_sources, all_dates], names=[\"Source\",\"Date\"])\n",
    "    df_full = df.set_index([\"Source\",\"Date\"]).reindex(idx).reset_index()\n",
    "    df_full[\"kwh\"] = df_full[\"kwh\"].fillna(0)\n",
    "    climate_cols = [c for c in df.columns if c not in [\"Source\",\"kwh\",\"Date\"]]\n",
    "    for c in climate_cols:\n",
    "        df_full[c] = df_full.groupby(\"Date\")[c].transform(\"first\")\n",
    "    return df_full\n",
    "\n",
    "# --- Prepare dataset ---\n",
    "df = merged_daily_df.copy()\n",
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "df = reindex_sources(df)\n",
    "df = add_features(df)\n",
    "df = add_lag_roll(df)\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "feature_cols = [\n",
    "    \"Temp_Mean\",\"Temp_Min\",\"Temp_Max\",\"Dewpoint_Mean\",\"Dewpoint_Min\",\"Dewpoint_Max\",\n",
    "    \"U_Wind_Mean\",\"V_Wind_Mean\",\"Precipitation_Sum\",\"Snowfall_Sum\",\"SnowCover_Mean\",\n",
    "    \"Temp_dew_diff\",\"wind_speed\",\"precip_snow_ratio\",\n",
    "] + [c for c in df.columns if c.startswith(\"lag_\") or c.startswith(\"roll_\")]\n",
    "\n",
    "X = df[feature_cols].values\n",
    "y = df[\"kwh\"].values\n",
    "\n",
    "# Scale features for neural net\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# --- Build MLP model ---\n",
    "def create_mlp(input_dim):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(128, activation=\"relu\", input_dim=input_dim))\n",
    "    model.add(layers.Dropout(0.2))\n",
    "    model.add(layers.Dense(64, activation=\"relu\"))\n",
    "    model.add(layers.Dropout(0.2))\n",
    "    model.add(layers.Dense(32, activation=\"relu\"))\n",
    "    model.add(layers.Dense(1, activation=\"relu\"))  # relu ensures no negative kwh\n",
    "    model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
    "    return model\n",
    "\n",
    "mlp = create_mlp(X_train.shape[1])\n",
    "\n",
    "# --- Callbacks: EarlyStopping + ReduceLROnPlateau ---\n",
    "early_stop = callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\", patience=5, restore_best_weights=True, verbose=0\n",
    ")\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\", factor=0.5, patience=3, min_lr=1e-6, verbose=0\n",
    ")\n",
    "\n",
    "history = mlp.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test,y_test),\n",
    "    epochs=100, batch_size=256, verbose=0,\n",
    "    callbacks=[early_stop, reduce_lr]\n",
    ")\n",
    "\n",
    "# Predict on test\n",
    "y_pred = mlp.predict(X_test, verbose=0).flatten()\n",
    "y_pred = np.maximum(0, y_pred)  # safety clip\n",
    "\n",
    "# --- Global Metrics ---\n",
    "print(\"=== Global Metrics ===\")\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "print(f\"Regression: RMSE={rmse:.3f}, MAPE={mape:.3f}\\n\")\n",
    "\n",
    "# --- Predict next month (extra data) ---\n",
    "extra = add_features(extra_month_df.copy())\n",
    "extra[\"Date\"] = pd.to_datetime(extra[\"Date\"])\n",
    "\n",
    "preds = []\n",
    "sources = df[\"Source\"].unique()\n",
    "for src in sources:\n",
    "    hist = df[df[\"Source\"]==src].copy()\n",
    "    extra_src = extra.copy()\n",
    "    extra_src[\"Source\"] = src\n",
    "    lag_hist = hist.tail(14).reset_index(drop=True)  # store enough history for lag_14\n",
    "    rows = []\n",
    "    for _, row in extra_src.iterrows():\n",
    "        r = row.to_dict()\n",
    "        for lag in [1,2,7,14]:\n",
    "            if len(lag_hist) >= lag:\n",
    "                r[f\"lag_{lag}\"] = lag_hist.loc[len(lag_hist)-lag,\"kwh\"]\n",
    "            else:\n",
    "                r[f\"lag_{lag}\"] = np.nan\n",
    "        for w in [3,7,14]:\n",
    "            if len(lag_hist) >= w:\n",
    "                r[f\"roll_mean_{w}\"] = lag_hist[\"kwh\"].iloc[-w:].mean()\n",
    "            else:\n",
    "                r[f\"roll_mean_{w}\"] = lag_hist[\"kwh\"].mean()\n",
    "        X_ex = pd.DataFrame([r])[feature_cols].fillna(0)\n",
    "        X_ex = scaler.transform(X_ex)\n",
    "        r[\"pred_kwh\"] = max(0, mlp.predict(X_ex, verbose=0)[0,0])\n",
    "        lag_hist = pd.concat([lag_hist, pd.DataFrame([{\"kwh\": r[\"pred_kwh\"]}])], ignore_index=True)\n",
    "        rows.append(r)\n",
    "    preds.append(pd.DataFrame(rows))\n",
    "preds_df = pd.concat(preds)\n",
    "\n",
    "\n",
    "\n",
    "sub = pd.read_csv(\"SampleSubmission.csv\")\n",
    "sub[\"Date\"] = sub[\"ID\"].apply(lambda x: x.split(\"_\")[0])\n",
    "sub[\"Source\"] = sub[\"ID\"].apply(lambda x: \"_\".join(x.split(\"_\")[1:]))\n",
    "sub[\"Date\"] = pd.to_datetime(sub[\"Date\"])\n",
    "\n",
    "preds_out = preds_df.copy()\n",
    "preds_out[\"Date\"] = pd.to_datetime(preds_out[\"Date\"])\n",
    "\n",
    "submission = sub.merge(preds_out[[\"Date\",\"Source\",\"pred_kwh\"]], on=[\"Date\",\"Source\"], how=\"left\")\n",
    "submission = submission[[\"ID\",\"pred_kwh\"]]\n",
    "submission.to_csv(\"MySubmission_7_MLP.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311d22b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on test\n",
    "y_pred_train = mlp.predict(X_train, verbose=0).flatten()\n",
    "y_pred_train = np.maximum(0, y_pred_train)  # safety clip\n",
    "\n",
    "# --- Global Metrics ---\n",
    "print(\"=== Global Metrics ===\")\n",
    "rmse = mean_squared_error(y_train, y_pred_train, squared=False)\n",
    "\n",
    "print(f\"Regression: RMSE train={rmse:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9822151a",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc1dfa1",
   "metadata": {},
   "source": [
    "-----------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032517a9",
   "metadata": {},
   "source": [
    "# 5th Approach:  Huber loss instead of MSE ------> Zindi 4.390272303 1st Place !\n",
    "\n",
    "- RMSE punishes big spikes a lot â†’ in our data (zeros + bursts), that can dominate training.\n",
    "\n",
    "- Huber loss is a mix: it acts like MSE for small errors, and like MAE for large errors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe767f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Feature Engineering ---\n",
    "def add_features(df):\n",
    "    df = df.copy()\n",
    "    df[\"Temp_dew_diff\"] = df[\"Temp_Mean\"] - df[\"Dewpoint_Mean\"]\n",
    "    df[\"wind_speed\"] = np.sqrt(df[\"U_Wind_Mean\"]**2 + df[\"V_Wind_Mean\"]**2)\n",
    "    df[\"precip_snow_ratio\"] = df[\"Precipitation_Sum\"] / (df[\"Snowfall_Sum\"] + 1e-6)\n",
    "    return df\n",
    "\n",
    "def add_lag_roll(df, group_col=\"Source\", target_col=\"kwh\", lags=[1,2,7,14], windows=[3,7,14]):\n",
    "    df = df.sort_values([\"Source\",\"Date\"]).copy()\n",
    "    for lag in lags:\n",
    "        df[f\"lag_{lag}\"] = df.groupby(group_col)[target_col].shift(lag)\n",
    "    for w in windows:\n",
    "        df[f\"roll_mean_{w}\"] = df.groupby(group_col)[target_col].shift(1).rolling(w).mean()\n",
    "    return df\n",
    "\n",
    "# --- Step 1: Reindex all Sources across full date range ---\n",
    "def reindex_sources(df):\n",
    "    all_dates = pd.date_range(df[\"Date\"].min(), df[\"Date\"].max())\n",
    "    all_sources = df[\"Source\"].unique()\n",
    "    idx = pd.MultiIndex.from_product([all_sources, all_dates], names=[\"Source\",\"Date\"])\n",
    "    df_full = df.set_index([\"Source\",\"Date\"]).reindex(idx).reset_index()\n",
    "    df_full[\"kwh\"] = df_full[\"kwh\"].fillna(0)\n",
    "    climate_cols = [c for c in df.columns if c not in [\"Source\",\"kwh\",\"Date\"]]\n",
    "    for c in climate_cols:\n",
    "        df_full[c] = df_full.groupby(\"Date\")[c].transform(\"first\")\n",
    "    return df_full\n",
    "\n",
    "# --- Prepare dataset ---\n",
    "df = merged_daily_df.copy()\n",
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "df = reindex_sources(df)\n",
    "df = add_features(df)\n",
    "df = add_lag_roll(df)\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "feature_cols = [\n",
    "    \"Temp_Mean\",\"Temp_Min\",\"Temp_Max\",\"Dewpoint_Mean\",\"Dewpoint_Min\",\"Dewpoint_Max\",\n",
    "    \"U_Wind_Mean\",\"V_Wind_Mean\",\"Precipitation_Sum\",\"Snowfall_Sum\",\"SnowCover_Mean\",\n",
    "    \"Temp_dew_diff\",\"wind_speed\",\"precip_snow_ratio\",\n",
    "] + [c for c in df.columns if c.startswith(\"lag_\") or c.startswith(\"roll_\")]\n",
    "\n",
    "X = df[feature_cols].values\n",
    "y = df[\"kwh\"].values\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# --- Build MLP model (no BN, just Dropout) ---\n",
    "def create_mlp(input_dim):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(128, activation=\"relu\", input_dim=input_dim))\n",
    "    model.add(layers.Dropout(0.2))\n",
    "    model.add(layers.Dense(64, activation=\"relu\"))\n",
    "    model.add(layers.Dropout(0.2))\n",
    "    model.add(layers.Dense(32, activation=\"relu\"))\n",
    "    model.add(layers.Dense(1, activation=\"relu\"))  # ensures no negative kwh\n",
    "    \n",
    "    # ðŸ‘‡ Huber loss instead of MSE\n",
    "    model.compile(optimizer=\"adam\", loss=tf.keras.losses.Huber(), metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
    "    return model\n",
    "\n",
    "mlp = create_mlp(X_train.shape[1])\n",
    "\n",
    "# --- Callbacks ---\n",
    "early_stop = callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\", patience=5, restore_best_weights=True, verbose=0\n",
    ")\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\", factor=0.5, patience=3, min_lr=1e-6, verbose=0\n",
    ")\n",
    "\n",
    "# --- Train ---\n",
    "history = mlp.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test,y_test),\n",
    "    epochs=100, batch_size=256, verbose=0,\n",
    "    callbacks=[early_stop, reduce_lr]\n",
    ")\n",
    "\n",
    "# --- Predict on test ---\n",
    "y_pred = mlp.predict(X_test, verbose=0).flatten()\n",
    "y_pred = np.maximum(0, y_pred)\n",
    "\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "print(f\"MLP + Huber: RMSE={rmse:.3f}, MAPE={mape:.3f}\")\n",
    "\n",
    "# --- Predict next month (extra data) ---\n",
    "extra = add_features(extra_month_df.copy())\n",
    "extra[\"Date\"] = pd.to_datetime(extra[\"Date\"])\n",
    "\n",
    "preds = []\n",
    "sources = df[\"Source\"].unique()\n",
    "for src in sources:\n",
    "    hist = df[df[\"Source\"]==src].copy()\n",
    "    extra_src = extra.copy()\n",
    "    extra_src[\"Source\"] = src\n",
    "    lag_hist = hist.tail(30).reset_index(drop=True)\n",
    "    rows = []\n",
    "    for _, row in extra_src.iterrows():\n",
    "        r = row.to_dict()\n",
    "        for lag in [1,2,7,14]:\n",
    "            if len(lag_hist) >= lag:\n",
    "                r[f\"lag_{lag}\"] = lag_hist.loc[len(lag_hist)-lag,\"kwh\"]\n",
    "            else:\n",
    "                r[f\"lag_{lag}\"] = np.nan\n",
    "        for w in [3,7,14]:\n",
    "            if len(lag_hist) >= w:\n",
    "                r[f\"roll_mean_{w}\"] = lag_hist[\"kwh\"].iloc[-w:].mean()\n",
    "            else:\n",
    "                r[f\"roll_mean_{w}\"] = lag_hist[\"kwh\"].mean()\n",
    "        X_ex = pd.DataFrame([r])[feature_cols].fillna(0)\n",
    "        X_ex = scaler.transform(X_ex)\n",
    "        r[\"pred_kwh\"] = max(0, mlp.predict(X_ex, verbose=0)[0,0])\n",
    "        lag_hist = pd.concat([lag_hist, pd.DataFrame([{\"kwh\": r[\"pred_kwh\"]}])], ignore_index=True)\n",
    "        rows.append(r)\n",
    "    preds.append(pd.DataFrame(rows))\n",
    "preds_df = pd.concat(preds)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e785ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv(\"SampleSubmission.csv\")\n",
    "sub[\"Date\"] = sub[\"ID\"].apply(lambda x: x.split(\"_\")[0])\n",
    "sub[\"Source\"] = sub[\"ID\"].apply(lambda x: \"_\".join(x.split(\"_\")[1:]))\n",
    "sub[\"Date\"] = pd.to_datetime(sub[\"Date\"])\n",
    "\n",
    "preds_out = preds_df.copy()\n",
    "preds_out[\"Date\"] = pd.to_datetime(preds_out[\"Date\"])\n",
    "\n",
    "submission = sub.merge(preds_out[[\"Date\",\"Source\",\"pred_kwh\"]], on=[\"Date\",\"Source\"], how=\"left\")\n",
    "submission = submission[[\"ID\",\"pred_kwh\"]]\n",
    "submission.to_csv(\"MySubmission_9_MLP.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd282e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# --- Plot historical vs predictions for a few sources ---\n",
    "sample_sources = np.random.choice(df[\"Source\"].unique(), 3, replace=False)\n",
    "\n",
    "for src in sample_sources:\n",
    "    df_src = df[df[\"Source\"] == src].copy()\n",
    "    preds_src = preds_df[preds_df[\"Source\"] == src].copy()\n",
    "    \n",
    "    plt.figure(figsize=(12,5))\n",
    "    plt.plot(df_src[\"Date\"], df_src[\"kwh\"], label=\"Actual (history)\", alpha=0.7)\n",
    "    plt.plot(preds_src[\"Date\"], preds_src[\"pred_kwh\"], label=\"Predicted (extra month)\", alpha=0.7, linestyle=\"--\")\n",
    "    plt.title(f\"Source: {src}\")\n",
    "    plt.xlabel(\"Date\"); plt.ylabel(\"kWh\")\n",
    "    plt.legend(); plt.show()\n",
    "\n",
    "# --- Total demand (sum of all sources) ---\n",
    "df_total = df.groupby(\"Date\")[\"kwh\"].sum().reset_index()\n",
    "preds_total = preds_df.groupby(\"Date\")[\"pred_kwh\"].sum().reset_index()\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.plot(df_total[\"Date\"], df_total[\"kwh\"], label=\"Actual Total (history)\", alpha=0.7)\n",
    "plt.plot(preds_total[\"Date\"], preds_total[\"pred_kwh\"], label=\"Predicted Total (extra month)\", alpha=0.7, linestyle=\"--\")\n",
    "plt.title(\"Total Electricity Demand\")\n",
    "plt.xlabel(\"Date\"); plt.ylabel(\"Total kWh\")\n",
    "plt.legend(); plt.show()\n",
    "\n",
    "# ===================================================\n",
    "# --- Residual Analysis (using train/test predictions) ---\n",
    "# Compute residuals for test set\n",
    "residuals = y_test - y_pred\n",
    "\n",
    "# Residuals distribution\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.histplot(residuals, bins=50, kde=True)\n",
    "plt.title(\"Residual Distribution (Test Set)\")\n",
    "plt.xlabel(\"Residual (Actual - Predicted)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "# Scatter plot: Predicted vs Actual\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.scatter(y_test, y_pred, alpha=0.3)\n",
    "plt.plot([0, max(y_test)], [0, max(y_test)], \"r--\")  # perfect line\n",
    "plt.xlabel(\"Actual kWh\")\n",
    "plt.ylabel(\"Predicted kWh\")\n",
    "plt.title(\"Predicted vs Actual (Test Set)\")\n",
    "plt.show()\n",
    "\n",
    "# Residuals vs Predictions (check bias)\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.scatter(y_pred, residuals, alpha=0.3)\n",
    "plt.axhline(0, color=\"red\", linestyle=\"--\")\n",
    "plt.xlabel(\"Predicted kWh\")\n",
    "plt.ylabel(\"Residual (Actual - Predicted)\")\n",
    "plt.title(\"Residuals vs Predicted Values (Test Set)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69dba3f",
   "metadata": {},
   "source": [
    "--------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0108c3",
   "metadata": {},
   "source": [
    "# Okey, so we keep that model. Let us do some more performance assesment: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0e5e63",
   "metadata": {},
   "source": [
    "- Do random split (same as before) â†’ for competition metrics.\n",
    "\n",
    "- Do temporal split (last 30 days per source) â†’ to simulate forecasting and allow plotting of actual vs predicted as a time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73da736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Keep your existing feature engineering pipeline ---\n",
    "# df, X, y, feature_cols, scaler are already prepared\n",
    "# mlp model is already trained on RANDOM split\n",
    "\n",
    "# =======================================================\n",
    "# 1) Random Split (already done, used for Zindi scoring)\n",
    "# X_train, X_test, y_train, y_test = ...\n",
    "\n",
    "# =======================================================\n",
    "# 2) Temporal Validation Split\n",
    "\n",
    "df_sorted = df.sort_values([\"Source\", \"Date\"]).copy()\n",
    "\n",
    "temporal_val = []\n",
    "temporal_preds = []\n",
    "\n",
    "for src in df_sorted[\"Source\"].unique():\n",
    "    src_data = df_sorted[df_sorted[\"Source\"] == src].copy()\n",
    "    \n",
    "    if len(src_data) < 40:\n",
    "        continue  # skip too short series\n",
    "    \n",
    "    # last 30 days = validation\n",
    "    train_src = src_data.iloc[:-30]\n",
    "    val_src = src_data.iloc[-30:]\n",
    "    \n",
    "    X_val = scaler.transform(val_src[feature_cols].values)\n",
    "    y_val = val_src[\"kwh\"].values\n",
    "    y_hat = mlp.predict(X_val, verbose=0).flatten()\n",
    "    y_hat = np.maximum(0, y_hat)\n",
    "    \n",
    "    temporal_val.append(val_src.assign(pred=y_hat))\n",
    "    temporal_preds.append((y_val, y_hat))\n",
    "\n",
    "temporal_val_df = pd.concat(temporal_val)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb36a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================================================\n",
    "# 3) Plots for temporal validation\n",
    "sample_sources = np.random.choice(temporal_val_df[\"Source\"].unique(), 20, replace=False)\n",
    "\n",
    "for src in sample_sources:\n",
    "    src_val = temporal_val_df[temporal_val_df[\"Source\"] == src]\n",
    "    plt.figure(figsize=(12,5))\n",
    "    plt.plot(src_val[\"Date\"], src_val[\"kwh\"], label=\"Actual\", marker=\"o\")\n",
    "    plt.plot(src_val[\"Date\"], src_val[\"pred\"], label=\"Predicted\", marker=\"x\", linestyle=\"--\")\n",
    "    plt.title(f\"Temporal Validation: {src}\")\n",
    "    plt.xlabel(\"Date\"); plt.ylabel(\"kWh\")\n",
    "    plt.legend(); plt.show()\n",
    "\n",
    "# --- Total demand for validation period ---\n",
    "val_total = temporal_val_df.groupby(\"Date\")[[\"kwh\",\"pred\"]].sum().reset_index()\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.plot(val_total[\"Date\"], val_total[\"kwh\"], label=\"Actual Total\", marker=\"o\")\n",
    "plt.plot(val_total[\"Date\"], val_total[\"pred\"], label=\"Predicted Total\", marker=\"x\", linestyle=\"--\")\n",
    "plt.title(\"Temporal Validation: Total Electricity Demand\")\n",
    "plt.xlabel(\"Date\"); plt.ylabel(\"Total kWh\")\n",
    "plt.legend(); plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
