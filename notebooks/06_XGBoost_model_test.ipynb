{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74f9cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "rs = 3791"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7317cf",
   "metadata": {},
   "source": [
    "# Load & Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214b90f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/daily_merged_hydro_climate.csv\")  \n",
    "\n",
    "#ensure that date is an date_time object\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "#Renam Columns\n",
    "df = df.rename(columns={\n",
    "    \"Temperature (°C)_mean\": \"temp_mean\",\n",
    "    \"Temperature (°C)_min\": \"temp_min\",\n",
    "    \"Temperature (°C)_max\": \"temp_max\",\n",
    "    \"Dewpoint Temperature (°C)_mean\": \"dew_point_mean\",\n",
    "    \"Dewpoint Temperature (°C)_min\": \"dew_point_min\",\n",
    "    \"Dewpoint Temperature (°C)_max\": \"dew_point_max\",\n",
    "    \"U Wind Component (m/s)_mean\": \"u\",\n",
    "    \"V Wind Component (m/s)_mean\": \"v\",\n",
    "    \"Total Precipitation (mm)_sum\": \"tot_precip\",\n",
    "    \"Snowfall (mm)_sum\": \"snowfall\",\n",
    "    \"Snow Cover (%)_mean\": \"snow_cover\"\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ecd6c89",
   "metadata": {},
   "source": [
    "The data is missing some information for certain days within the time series. To prepare the data for modeling, we **reindex the dataset so that every Source has an entry for every Date** between the earliest and latest timestamp in the dataset. This ensures that the time series is complete and consistent across all sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32750144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the complete date range in the dataset\n",
    "all_dates = pd.date_range(df[\"date\"].min(), df[\"date\"].max())\n",
    "\n",
    "# Get all unique values for \"Source\" \n",
    "all_sources = df[\"Source\"].unique()\n",
    "\n",
    "#Build a MultiIndex with all combinations of Sources × Dates\n",
    "idx = pd.MultiIndex.from_product([all_sources, all_dates], names=[\"Source\",\"date\"])\n",
    "\n",
    "#Reindex the dataframe (row for every Source-Date combiation)\n",
    "df_full = df.set_index([\"Source\",\"date\"]).reindex(idx).reset_index()\n",
    "\n",
    "#Fill missing kWh values with 0 \n",
    "df_full[\"kwh_sum\"] = df_full[\"kwh_sum\"].fillna(0)\n",
    "\n",
    "#Identify all climate-related columns and forward values so every Source has the same climate data on a given Dat\n",
    "climate_cols = [c for c in df.columns if c not in [\"Source\",\"kwh_sum\",\"date\"]]\n",
    "for c in climate_cols:\n",
    "    df_full[c] = df_full.groupby(\"date\")[c].transform(\"first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d4d103",
   "metadata": {},
   "source": [
    "### Feature Engeneering\n",
    "\n",
    "To help the model capture **temporal dynamics** in the data, we generate lagged and rolling features for both energy consumption and climate variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3bf26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lags = [1, 2, 3]  # example: previous day, 2 days ago, 3 days ago, 1 week ago, 1 month agor   \n",
    "for lag in lags:\n",
    "    df[f'kwh_sum{lag}'] = df['kwh_sum'].shift(lag).fillna(0).astype(int)  #fill NaNs, generated durinng lag feature creatiion with 0\n",
    "\n",
    "lags = [7, 30]  # example: previous day, 2 days ago, 3 days ago, 1 week ago, 1 month agor   \n",
    "for lag in lags:\n",
    "    df[f'kwh_sum{lag}'] = df['kwh_sum'].rolling(window=30, min_periods=1).mean()   #fill NaNs, generated durinng lag feature creatiion with 0\n",
    "\n",
    "# moving average lags for relevant climate features\n",
    "df['temp_mean_30d'] = df['temp_mean'].rolling(window=30, min_periods=1).mean()    #long memory\n",
    "df['temp_mean_30d'] = df['temp_mean_30d'].fillna(0)\n",
    "\n",
    "df['dew_point_mean_30d'] = df['dew_point_mean'].rolling(window=7, min_periods=1).mean()    #long memory\n",
    "df['dew_point_mean_30d'] = df['dew_point_mean_30d'].fillna(0)\n",
    "\n",
    "df['tot_precip_30d'] = df['tot_precip'].rolling(window=30, min_periods=1).mean()    #long memory\n",
    "df['tot_precip_30d'] = df['tot_precip_30d'].fillna(0)\n",
    "\n",
    "df['snowfall_30d'] = df['snowfall'].rolling(window=30, min_periods=1).mean()    #long memory\n",
    "df['snowfall_30d'] = df['snowfall_30d'].fillna(0)\n",
    "\n",
    "df['snow_cover_30d'] = df['snow_cover'].rolling(window=30, min_periods=1).mean()    #long memory\n",
    "df['snow_cover_30d'] = df['snow_cover_30d'].fillna(0)        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd695bfd",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec94fe1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from timeit import default_timer as timer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import r2_score, mean_squared_error   \n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6736c15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[[\"temp_mean\",\n",
    "    \"dew_point_mean\",\n",
    "    \"u\",\n",
    "    \"v\",\n",
    "    \"tot_precip\",\n",
    "    \"snowfall\",\n",
    "    \"snow_cover\",\n",
    "    'kwh_sum1',\n",
    "    'kwh_sum2',\n",
    "    'kwh_sum3',\n",
    "    'kwh_sum7',\n",
    "    'kwh_sum30',\n",
    "    'temp_mean_30d',\n",
    "    'dew_point_mean_30d',\n",
    "    'tot_precip_30d',\n",
    "    'snowfall_30d',\n",
    "    'snow_cover_30d',\n",
    "    'consumer_device']]   \n",
    "y = df[\"kwh_sum\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd227940",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=rs, shuffle=True, stratify=X[\"consumer_device\"])\n",
    "\n",
    "#drop consumer_device\n",
    "\n",
    "X_train.drop('consumer_device', axis = 1, inplace = True)\n",
    "X_test.drop('consumer_device', axis = 1, inplace = True)\n",
    "\n",
    "#safe features used for model training \n",
    "feature_cols = X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e31e54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model instance\n",
    "bst = XGBRegressor()     \n",
    "\n",
    "bst.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176c131d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test set \n",
    "preds_test = bst.predict(X_test)\n",
    "print(preds_test)\n",
    "\n",
    "# Make predictions on train set set \n",
    "preds_train = bst.predict(X_train)\n",
    "print(preds_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6fc387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on train data\n",
    "r2_train = r2_score(y_train, preds_train)\n",
    "rmse_train = np.sqrt(mean_squared_error(y_train, preds_train))\n",
    "\n",
    "print('r squared on training data is: ', round(r2_train,3))\n",
    "print(\"RMSE on training data is: \", round(rmse_train,3))\n",
    "\n",
    "# Evaluate the model on test data\n",
    "r2 = r2_score(y_test, preds_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, preds_test))\n",
    "\n",
    "print('r squared for test data is:', round(r2,3))\n",
    "print(\"RMSE for test data is: \", round(rmse,3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b222190",
   "metadata": {},
   "source": [
    "The target variable (`kwh_sum`) contains many **zero values**, including long periods of several weeks with no energy availability. Consequently:   \n",
    "\n",
    "*  **R² becomes misleading**  \n",
    "* **RMSE remains meaningful** \n",
    "\n",
    "As the dataset is dominated by zeros, R² may not be a fair indicator of model quality. RMSE (or other absolute error measures) in this case provides a more reliable view of predictive performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9606cd",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038203b6",
   "metadata": {},
   "source": [
    "We first start with a broader search to identify the most influential parameters and their approximate ranges, reducing the search space for the next stage. The target is to optimize the RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27e480b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sefine Paarameters for tuning\n",
    "param_dist_stage1 = {\n",
    "    \"n_estimators\": [100, 500, 1000],\n",
    "    \"max_depth\": [3, 6, 9],       #Shallow trees generalize better, deep trees tend to overfit\n",
    "    \"learning_rate\": [0.01, 0.05, 0.1],\n",
    "    \"subsample\": [0.6, 0.8, 1.0],   # Adds randomness, prevents overfitting\n",
    "    \"colsample_bytree\": [0.5, 0.7, 1.0],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34b9ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(estimator=bst,\n",
    "                    param_grid=param_dist_stage1,\n",
    "                    cv=4,\n",
    "                    scoring='neg_root_mean_squared_error',  # or 'r2' \n",
    "                    verbose=1,\n",
    "                    n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40be069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit object to data\n",
    "start = timer()\n",
    "grid.fit(X_train, y_train)\n",
    "end = timer()\n",
    "gs_time = end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d97092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best score\n",
    "print('Best score:', round(grid.best_score_, 3))\n",
    "\n",
    "# Best parameters\n",
    "best_params_stage1  = grid.best_params_\n",
    "print('Best parameters:', best_params_stage1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ac0b81",
   "metadata": {},
   "source": [
    "The hyperparameter tuning was run with the goal to optimize `scoring='neg_root_mean_squared_error'`. In scikit-learn, all scoring functions are defined so that **higher values mean better performance**. Since RMSE is an error metric (where *lower is better*), scikit-learn simply returns the **negative RMSE** to stay consistent with this convention. Hence, for the actual best RMSSE after tuning, just remove the negative sign.\n",
    "\n",
    "The tuned model achieved a cross-validated RMSE of **3.499**, which is an improvement over both the baseline (~3.8) and the benchmark (~4.5).\n",
    "\n",
    "The best combination of parameters is:\n",
    "* 'colsample_bytree' = 0.7\n",
    "* 'learning_rate'= 0.01\n",
    "* 'max_depth'= 3\n",
    "* 'n_estimators'= 500\n",
    "* 'subsample'= 1.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abfcb71",
   "metadata": {},
   "source": [
    "### Further Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74c183e",
   "metadata": {},
   "source": [
    "A second, fine-tuned grid search focused on less influential parameters while keeping the previously optimized parameters fixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932ef7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sefine Paarameters for tuning\n",
    "param_dist_stage2  = {   \n",
    "    \"gamma\": [0, 0.1, 0.5],\n",
    "    \"reg_alpha\": [0, 0.1, 1],\n",
    "    \"reg_lambda\": [1, 2, 5]\n",
    "}\n",
    "\n",
    "# Merge: keep best values for stage 1, vary the new ones\n",
    "param_dist_fTune = {**best_params_stage1, **param_dist_stage2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc338b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "param_dist_fTune = { \"n_estimators\": [500],\n",
    "                    \"max_depth\": [3],\n",
    "                    \"learning_rate\": [0.01],\n",
    "                    \"subsample\": [1.0],\n",
    "                    \"colsample_bytree\": [0.7],\n",
    "                    \"gamma\": [0, 0.1, 0.5],\n",
    "                    \"reg_alpha\": [0, 0.1, 1],\n",
    "                    \"reg_lambda\": [1, 2, 5] }\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc818ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(estimator=bst,\n",
    "                    param_grid=param_dist_fTune,\n",
    "                    cv=4,\n",
    "                    scoring='neg_root_mean_squared_error',  # or 'r2' \n",
    "                    verbose=1,\n",
    "                    n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a20093e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit object to data\n",
    "start = timer()\n",
    "grid.fit(X_train, y_train)\n",
    "end = timer()\n",
    "gs_time = end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f952680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best score\n",
    "print('Best score:', round(grid.best_score_, 3))\n",
    "\n",
    "# Best parameters\n",
    "best_params_stage2  = grid.best_params_\n",
    "print('Best parameters:', best_params_stage2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c39d2da",
   "metadata": {},
   "source": [
    "The small change indicates that the model was already close to optimal with the first-stage parameters, and further tuning of regularization had only minimal impact."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981cd966",
   "metadata": {},
   "source": [
    "### Train Optimized Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9419663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model instance using the best parameters from tuning\n",
    "bst = XGBRegressor(**grid.best_params_)\n",
    "\n",
    "# Train on your full training data\n",
    "bst.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677692fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test set \n",
    "preds_test = bst.predict(X_test)\n",
    "print(preds_test)\n",
    "\n",
    "# Make predictions on train set set \n",
    "preds_train = bst.predict(X_train)\n",
    "print(preds_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f98a6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on train data\n",
    "r2_train = r2_score(y_train, preds_train)\n",
    "rmse_train = np.sqrt(mean_squared_error(y_train, preds_train))\n",
    "\n",
    "print('r squared on training data is: ', round(r2_train,3))\n",
    "print(\"RMSE on training data is: \", round(rmse_train,3))\n",
    "\n",
    "# Evaluate the model on test data\n",
    "r2 = r2_score(y_test, preds_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, preds_test))\n",
    "\n",
    "print('r squared for test data is:', round(r2,3))\n",
    "print(\"RMSE for test data is: \", round(rmse,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22866667",
   "metadata": {},
   "source": [
    "# Prepare Submission File for Zindi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9e1734",
   "metadata": {},
   "source": [
    "In thiss section, the final model is used to generate sequential predictions for each source over the validation period. Lag and rolling features weraree dynamically updated after each predicted day to simulate real-world forecasting. The predictions were merged with the sample submission template to produce a properly formatted CSV file ready for upload to Zindi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cfeb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Load the climate data\n",
    "\n",
    "file_path = \"../data/Kalam Climate Data.xlsx\"   \n",
    "climate_df = pd.read_excel(file_path)\n",
    "\n",
    "climate_df['Date Time'] = pd.to_datetime(climate_df['Date Time'])\n",
    "\n",
    "# Extract date\n",
    "\n",
    "climate_df['date'] = climate_df['Date Time'].dt.date\n",
    "\n",
    "# Daily aggregation\n",
    "daily_climate = climate_df.groupby('date').agg({\n",
    "    'Temperature (°C)': ['mean', 'min', 'max'],\n",
    "    'Dewpoint Temperature (°C)': ['mean', 'min', 'max'],\n",
    "    'U Wind Component (m/s)': 'mean',\n",
    "    'V Wind Component (m/s)': 'mean',\n",
    "    'Total Precipitation (mm)': 'sum',\n",
    "    'Snowfall (mm)': 'sum',\n",
    "    'Snow Cover (%)': 'mean'\n",
    "})\n",
    "\n",
    "# Flatten columns\n",
    "daily_climate.columns = ['_'.join(col).strip() for col in daily_climate.columns.values]\n",
    "\n",
    "# Reset index\n",
    "daily_climate = daily_climate.reset_index()\n",
    "\n",
    "# Define start and end dates\n",
    "start_date = pd.to_datetime('2024-09-24')\n",
    "end_date   = pd.to_datetime('2024-10-24')\n",
    "\n",
    "daily_climate[\"date\"] = pd.to_datetime(daily_climate[\"date\"])\n",
    "\n",
    "# Filter DataFrame\n",
    "extra_month_df = daily_climate[(daily_climate['date'] >= start_date) & (daily_climate['date'] <= end_date)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fc8692",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename colums to match your feature_cols\n",
    "extra_month_df = extra_month_df.rename(columns={\n",
    "    'Temperature (°C)_mean': 'temp_mean',\n",
    "    'Dewpoint Temperature (°C)_mean': 'dew_point_mean',\n",
    "    'U Wind Component (m/s)_mean': 'u',\n",
    "    'V Wind Component (m/s)_mean': 'v',\n",
    "    'Total Precipitation (mm)_sum': 'tot_precip',\n",
    "    'Snowfall (mm)_sum': 'snowfall',\n",
    "    'Snow Cover (%)_mean': 'snow_cover'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2241d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "\n",
    "for src in df[\"Source\"].unique():\n",
    "    hist = df[df[\"Source\"]==src].copy()\n",
    "    extra_src = extra_month_df.copy()\n",
    "    extra_src[\"Source\"] = src\n",
    "    \n",
    "    lag_hist = hist.tail(30).reset_index(drop=True)\n",
    "    rows = []\n",
    "    \n",
    "    for _, row in extra_src.iterrows():\n",
    "        r = row.copy()\n",
    "        \n",
    "        # Lag features for target\n",
    "        for lag in [1,2,7,14]:\n",
    "            r[f\"kwh_sum{lag}\"] = lag_hist[\"kwh_sum\"].iloc[-lag] if len(lag_hist) >= lag else 0\n",
    "        \n",
    "        # Rolling mean features for target\n",
    "        for w in [3,7,14]:\n",
    "            r[f\"roll_mean_{w}\"] = lag_hist[\"kwh_sum\"].iloc[-w:].mean() if len(lag_hist) >= w else lag_hist[\"kwh_sum\"].mean()\n",
    "        \n",
    "        # Add rolling averages for climate features (30d) if needed\n",
    "        for feat in [\"temp_mean\",\"dew_point_mean\",\"tot_precip\",\"snowfall\",\"snow_cover\"]:\n",
    "            r[f\"{feat}_30d\"] = lag_hist[feat].iloc[-30:].mean() if len(lag_hist) >= 30 else lag_hist[feat].mean()\n",
    "        \n",
    "        # Ensure all features exist\n",
    "        X_ex = pd.DataFrame([r])\n",
    "        for col in feature_cols:\n",
    "            if col not in X_ex.columns:\n",
    "                X_ex[col] = 0\n",
    "        X_ex = X_ex[feature_cols]\n",
    "        \n",
    "        # Predict\n",
    "        r[\"pred_kwh\"] = max(0, bst.predict(X_ex)[0])\n",
    "        \n",
    "        # Update history\n",
    "        lag_hist = pd.concat([lag_hist, pd.DataFrame([{\"kwh_sum\": r[\"pred_kwh\"], **{c: r[c] for c in [\"temp_mean\",\"dew_point_mean\",\"u\",\"v\",\"tot_precip\",\"snowfall\",\"snow_cover\"]}}])], ignore_index=True)\n",
    "        \n",
    "        rows.append(r)\n",
    "    \n",
    "    preds.append(pd.DataFrame(rows))\n",
    "\n",
    "preds_df = pd.concat(preds).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd82bf3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_df = preds_df.rename(columns={\"date\": \"Date\"}) #Done to follow the conventions of the Zindii template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445507f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv(\"../data/SampleSubmission.csv\")\n",
    "sub[\"Date\"] = sub[\"ID\"].apply(lambda x: x.split(\"_\")[0])\n",
    "sub[\"Source\"] = sub[\"ID\"].apply(lambda x: \"_\".join(x.split(\"_\")[1:]))\n",
    "sub[\"Date\"] = pd.to_datetime(sub[\"Date\"])\n",
    "\n",
    "preds_out = preds_df.copy()\n",
    "preds_out[\"Date\"] = pd.to_datetime(preds_out[\"Date\"])\n",
    "\n",
    "submission = sub.merge(preds_out[[\"Date\",\"Source\",\"pred_kwh\"]], on=[\"Date\",\"Source\"], how=\"left\")\n",
    "submission = submission[[\"ID\",\"pred_kwh\"]]\n",
    "submission.to_csv(\"MySubmission_XGBoost_V1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b8164c",
   "metadata": {},
   "source": [
    "### FAZIT – Overall Model Performance\n",
    "\n",
    "When testing the model on new, unnknown climate data, itt achieved a Zindi privat score of 7.618465289, which would have ranked it on position 321. The current model is functional and can generate submission files, but its predictive accuracy is limited. Further pursuit of this particular modeling approach is unlikely to yield substantial gains."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (WattsUP_Capstone)",
   "language": "python",
   "name": "wattsup_capstone"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
